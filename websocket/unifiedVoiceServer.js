const WebSocket = require("ws");

// Load API keys from environment variables
const API_KEYS = {
  deepgram: process.env.DEEPGRAM_API_KEY,
  sarvam: process.env.SARVAM_API_KEY,
  openai: process.env.OPENAI_API_KEY,
};

// Validate API keys
if (!API_KEYS.deepgram || !API_KEYS.sarvam || !API_KEYS.openai) {
  console.error("‚ùå Missing required API keys in environment variables");
  console.error("Required: DEEPGRAM_API_KEY, SARVAM_API_KEY, OPENAI_API_KEY");
  process.exit(1);
}

const fetch = globalThis.fetch || require("node-fetch");

// Performance timing helper
const createTimer = (label) => {
  const start = Date.now();
  return {
    start,
    end: () => Date.now() - start,
    checkpoint: (checkpointName) => Date.now() - start,
  };
};

// Language mappings
const LANGUAGE_MAPPING = {
  hi: "hi-IN",
  en: "en-IN",
  bn: "bn-IN",
  te: "te-IN",
  ta: "ta-IN",
  mr: "mr-IN",
  gu: "gu-IN",
  kn: "kn-IN",
  ml: "ml-IN",
  pa: "pa-IN",
  or: "or-IN",
  as: "as-IN",
  ur: "ur-IN",
};

// Get language codes for different services
const getSarvamLanguage = (detectedLang, defaultLang = "hi") => {
  const lang = detectedLang?.toLowerCase() || defaultLang;
  return LANGUAGE_MAPPING[lang] || "hi-IN";
};

const getDeepgramLanguage = (detectedLang, defaultLang = "hi") => {
  const lang = detectedLang?.toLowerCase() || defaultLang;
  if (lang === "hi") return "hi";
  if (lang === "en") return "en-IN";
  return lang;
};

// Valid Sarvam voice options
const VALID_SARVAM_VOICES = ["meera", "pavithra", "arvind", "amol", "maya"];

const getValidSarvamVoice = (voiceSelection = "pavithra") => {
  if (VALID_SARVAM_VOICES.includes(voiceSelection)) {
    return voiceSelection;
  }
  
  const voiceMapping = {
    "male-professional": "arvind",
    "female-professional": "pavithra",
    "male-friendly": "amol",
    "female-friendly": "maya",
    neutral: "pavithra",
    default: "pavithra",
  };
  
  return voiceMapping[voiceSelection] || "pavithra";
};

// Split text into sentences
const splitIntoSentences = (text) => {
  return text.match(/[^.!?]+[.!?]+|[^.!?]+$/g) || [text];
};

// Utility function to convert Buffer to Python bytes string for SIP
const bufferToPythonBytesString = (buffer) => {
  let result = "b'";
  for (let i = 0; i < buffer.length; i++) {
    const byte = buffer[i];
    if (byte >= 32 && byte <= 126 && byte !== 92 && byte !== 39) {
      result += String.fromCharCode(byte);
    } else {
      result += "\\x" + byte.toString(16).padStart(2, "0");
    }
  }
  result += "'";
  return result;
};

// Basic configuration
const DEFAULT_CONFIG = {
  agentName: "‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§∏‡§π‡§æ‡§Ø‡§ï",
  language: "hi",
  voiceSelection: "pavithra",
  firstMessage: "‡§®‡§Æ‡§∏‡•ç‡§§‡•á! ‡§Æ‡•à‡§Ç ‡§Ü‡§™‡§ï‡•Ä ‡§∏‡§π‡§æ‡§Ø‡§§‡§æ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ø‡§π‡§æ‡§Å ‡§π‡•Ç‡§Å‡•§ ‡§Ü‡§™ ‡§ï‡•à‡§∏‡•á ‡§π‡•à‡§Ç?",
  personality: "friendly",
  category: "customer service",
  contextMemory: "customer service conversation in Hindi",
};

// Language detection using OpenAI
const detectLanguage = async (text) => {
  const timer = createTimer("LANGUAGE_DETECTION");
  
  try {
    if (!API_KEYS.openai || !text.trim()) {
      timer.end();
      return "hi";
    }

    const requestBody = {
      model: "gpt-4o-mini",
      messages: [
        {
          role: "system",
          content: `You are a language detector. Detect the language of the given text and respond with just the language code (hi for Hindi, en for English, bn for Bengali, te for Telugu, ta for Tamil, mr for Marathi, gu for Gujarati, kn for Kannada, ml for Malayalam, pa for Punjabi, or for Odia, as for Assamese, ur for Urdu). If you're unsure or the text is mixed, respond with the dominant language. Only respond with the language code, nothing else.`,
        },
        {
          role: "user",
          content: text,
        },
      ],
      max_tokens: 10,
      temperature: 0.1,
    };

    const response = await fetch("https://api.openai.com/v1/chat/completions", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        Authorization: `Bearer ${API_KEYS.openai}`,
      },
      body: JSON.stringify(requestBody),
    });

    if (!response.ok) {
      console.error(`‚ùå [LANGUAGE_DETECT] OpenAI API error: ${response.status}`);
      timer.end();
      return "hi";
    }

    const data = await response.json();
    if (data.choices && data.choices[0] && data.choices[0].message) {
      const detectedLang = data.choices[0].message.content.trim().toLowerCase();
      console.log(`üîç [LANGUAGE_DETECT] Detected: ${detectedLang} (${timer.end()}ms)`);
      return detectedLang;
    }

    timer.end();
    return "hi";
  } catch (error) {
    console.error(`‚ùå [LANGUAGE_DETECT] Error: ${error.message}`);
    timer.end();
    return "hi";
  }
};

// OpenAI LLM processing
const processWithOpenAI = async (userMessage, conversationHistory, currentLanguage) => {
  const timer = createTimer("OPENAI_PROCESSING");
  
  try {
    if (!API_KEYS.openai || !userMessage.trim()) {
      timer.end();
      return null;
    }

    const systemPrompt = `You are ${DEFAULT_CONFIG.agentName}, a helpful Hindi voice assistant for customer service.

LANGUAGE INSTRUCTIONS:
- Default language: Hindi (hi)
- Current user language: ${currentLanguage || "hi"}
- Always respond in the same language as the user
- Keep responses short and conversational (1-2 sentences max)
- Be helpful and friendly
- Context: ${DEFAULT_CONFIG.contextMemory}

RESPONSE GUIDELINES:
- Maximum 150 characters
- Natural conversational tone
- Direct and helpful responses`;

    const messages = [
      { role: "system", content: systemPrompt },
      ...conversationHistory.slice(-6), // Keep last 6 messages for context
      { role: "user", content: userMessage }
    ];

    const requestBody = {
      model: "gpt-4o-mini",
      messages: messages,
      max_tokens: 100,
      temperature: 0.7,
    };

    const response = await fetch("https://api.openai.com/v1/chat/completions", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        Authorization: `Bearer ${API_KEYS.openai}`,
      },
      body: JSON.stringify(requestBody),
    });

    if (!response.ok) {
      console.error(`‚ùå [OPENAI] API error: ${response.status}`);
      timer.end();
      return null;
    }

    const data = await response.json();
    if (data.choices && data.choices[0] && data.choices[0].message) {
      const openaiResponse = data.choices[0].message.content.trim();
      console.log(`ü§ñ [OPENAI] Response: "${openaiResponse}" (${timer.end()}ms)`);
      return openaiResponse;
    }

    timer.end();
    return null;
  } catch (error) {
    console.error(`‚ùå [OPENAI] Error: ${error.message}`);
    timer.end();
    return null;
  }
};

// Sarvam TTS with chunked audio streaming (20ms-100ms chunks)
const synthesizeWithSarvam = async (text, language, ws, sessionId) => {
  const timer = createTimer("SARVAM_TTS_TOTAL");
  try {
    if (!API_KEYS.sarvam || !text.trim()) {
      timer.end();
      return;
    }
    const validVoice = getValidSarvamVoice(DEFAULT_CONFIG.voiceSelection);
    const sarvamLanguage = getSarvamLanguage(language);
    console.log(`üéµ [SARVAM] Starting TTS: "${text}" (${sarvamLanguage}, ${validVoice})`);
    const requestBody = {
      inputs: [text],
      target_language_code: sarvamLanguage,
      speaker: validVoice,
      pitch: 0,
      pace: 1.0,
      loudness: 1.0,
      speech_sample_rate: 22050,
      enable_preprocessing: false,
      model: "bulbul:v1",
    };
    const apiTimer = createTimer("SARVAM_API_CALL");
    const response = await fetch("https://api.sarvam.ai/text-to-speech", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        "API-Subscription-Key": API_KEYS.sarvam,
      },
      body: JSON.stringify(requestBody),
    });
    if (!response.ok) {
      const errorText = await response.text();
      console.error(`‚ùå [SARVAM] API error: ${response.status}`, errorText);
      timer.end();
      throw new Error(`Sarvam API error: ${response.status} - ${errorText}`);
    }
    const responseData = await response.json();
    apiTimer.end();
    console.log("üü¢ [SARVAM] API response:", JSON.stringify(responseData));
    if (!responseData.audios || responseData.audios.length === 0) {
      console.error("‚ùå [SARVAM] No audio data received");
      timer.end();
      throw new Error("Sarvam TTS: No audio data received");
    }
    const audioBase64 = responseData.audios[0];
    const audioBuffer = Buffer.from(audioBase64, "base64");
    // Split audio into chunks between 160 bytes (20ms) and 8000 bytes (100ms)
    const chunkSize = 1600; // ~20ms worth of audio data for 22050 Hz
    const totalChunks = Math.ceil(audioBuffer.length / chunkSize);
    console.log(`üì¶ [SARVAM] Splitting ${audioBuffer.length} bytes into ${totalChunks} chunks`);
    // Use outboundStreamSid for streamSid
    let streamSid = outboundStreamSid || sessionId || "outbound";
    // Send audio in chunks as 'media' events
    for (let i = 0; i < totalChunks; i++) {
      const start = i * chunkSize;
      const end = Math.min(start + chunkSize, audioBuffer.length);
      const chunk = audioBuffer.slice(start, end);
      if (chunk.length >= 160 && chunk.length <= 8000) {
        const base64Payload = chunk.toString("base64");
        const now = Date.now();
        const outboundMediaMsg = {
          event: "media",
          sequenceNumber: outboundSequenceNumber++,
          media: {
            track: "outbound",
            chunk: outboundChunkNumber++,
            timestamp: now,
            payload: base64Payload,
          },
          streamSid: streamSid,
        };
        if (ws.readyState === WebSocket.OPEN) {
          ws.send(JSON.stringify(outboundMediaMsg));
          console.log(`üì§ [SARVAM->CLIENT] Sent outbound chunk ${outboundMediaMsg.media.chunk}/${totalChunks} (bytes: ${chunk.length}, seq: ${outboundMediaMsg.sequenceNumber}, streamSid: ${streamSid})`);
        }
        // Small delay between chunks to simulate streaming
        await new Promise(resolve => setTimeout(resolve, 50));
      }
    }
    // Send completion signal (optional, can keep ai_response_complete for legacy clients)
    if (ws.readyState === WebSocket.OPEN) {
      ws.send(JSON.stringify({
        type: "ai_response_complete",
        session_id: sessionId,
        total_chunks: totalChunks,
        total_audio_bytes: audioBuffer.length,
      }));
    }
    console.log(`‚úÖ [SARVAM] TTS completed: ${totalChunks} chunks, ${audioBuffer.length} bytes (${timer.end()}ms)`);
  } catch (error) {
    console.error(`‚ùå [SARVAM] Error: ${error.message}`);
    timer.end();
    throw error;
  }
};

// Main WebSocket server setup
const setupUnifiedVoiceServer  = (wss) => {
  console.log("üöÄ [SERVER] Streamlined Voice Server started");

  wss.on("connection", (ws, req) => {
    console.log("üîó [CONNECTION] New WebSocket connection");

    // Session state
    let sessionId = null;
    let currentLanguage = "hi";
    let detectedLanguage = "hi";
    let conversationHistory = [];
    let isProcessing = false;
    let userUtteranceBuffer = "";
    let lastProcessedText = "";

    // Deepgram WebSocket connection
    let deepgramWs = null;
    let deepgramReady = false;

    // Outbound audio streaming state
    let outboundSequenceNumber = 1;
    let outboundChunkNumber = 1;
    let outboundStreamSid = null;

    // Buffer for audio chunks before Deepgram is ready
    let deepgramAudioBufferQueue = [];

    // Send initial greeting
    const sendInitialGreeting = async () => {
      console.log("üëã [GREETING] Sending initial greeting");
      // Send greeting text first
      if (ws.readyState === WebSocket.OPEN) {
        ws.send(JSON.stringify({
          type: "greeting_text",
          text: DEFAULT_CONFIG.firstMessage,
          language: currentLanguage,
          timestamp: new Date().toISOString(),
        }));
      }
      // Generate and send greeting audio
      console.log("üîä [GREETING] Calling Sarvam TTS for greeting...");
      try {
        await synthesizeWithSarvam(DEFAULT_CONFIG.firstMessage, currentLanguage, ws, sessionId);
        console.log("üîä [GREETING] Sarvam TTS for greeting completed and sent.");
      } catch (err) {
        console.error("‚ùå [GREETING] Sarvam TTS failed:", err);
      }
    };

    // Connect to Deepgram
    const connectToDeepgram = async () => {
      const timer = createTimer("DEEPGRAM_CONNECTION");
      try {
        if (!API_KEYS.deepgram) {
          throw new Error("Deepgram API key not available");
        }
        console.log("üîå [DEEPGRAM] Attempting to connect to Deepgram WebSocket...");
        const deepgramLanguage = getDeepgramLanguage(currentLanguage);
        const deepgramUrl = new URL("wss://api.deepgram.com/v1/listen");
        deepgramUrl.searchParams.append("sample_rate", "8000");
        deepgramUrl.searchParams.append("channels", "1");
        deepgramUrl.searchParams.append("encoding", "linear16");
        deepgramUrl.searchParams.append("model", "nova-2");
        deepgramUrl.searchParams.append("language", deepgramLanguage);
        deepgramUrl.searchParams.append("interim_results", "true");
        deepgramUrl.searchParams.append("smart_format", "true");
        deepgramUrl.searchParams.append("endpointing", "300");
        deepgramWs = new WebSocket(deepgramUrl.toString(), {
          headers: { Authorization: `Token ${API_KEYS.deepgram}` },
        });
        deepgramWs.onopen = () => {
          deepgramReady = true;
          console.log(`‚úÖ [DEEPGRAM] Connected (${timer.end()}ms)`);
          // Send all buffered audio
          if (deepgramAudioBufferQueue.length > 0) {
            console.log(`‚è© [DEEPGRAM] Sending ${deepgramAudioBufferQueue.length} buffered audio chunks to Deepgram...`);
            for (const buf of deepgramAudioBufferQueue) {
              try {
                deepgramWs.send(buf);
                console.log("‚úÖ [DEEPGRAM] Sent buffered audio chunk to Deepgram");
              } catch (err) {
                console.error("‚ùå [DEEPGRAM] Failed to send buffered audio chunk:", err);
              }
            }
            deepgramAudioBufferQueue = [];
          }
        };
        deepgramWs.onmessage = async (event) => {
          try {
            const data = JSON.parse(event.data);
            await handleDeepgramResponse(data);
          } catch (error) {
            console.error("‚ùå [DEEPGRAM] Parse error:", error.message);
          }
        };
        deepgramWs.onerror = (error) => {
          console.error("‚ùå [DEEPGRAM] Connection error:", error);
          deepgramReady = false;
        };
        deepgramWs.onclose = () => {
          console.log("üîå [DEEPGRAM] Connection closed");
          deepgramReady = false;
        };
      } catch (error) {
        console.error("‚ùå [DEEPGRAM] Setup error:", error.message);
        timer.end();
      }
    };

    // Process user utterance
    const processUserUtterance = async (text) => {
      if (!text.trim() || isProcessing || text === lastProcessedText) {
        return;
      }

      isProcessing = true;
      lastProcessedText = text;
      const timer = createTimer("USER_UTTERANCE_PROCESSING");

      try {
        // Step 1: Detect language
        const newDetectedLanguage = await detectLanguage(text);
        if (newDetectedLanguage !== detectedLanguage) {
          detectedLanguage = newDetectedLanguage;
          console.log(`üîÑ [LANGUAGE] Changed to: ${detectedLanguage}`);
        }

        // Step 2: Process with OpenAI
        const response = await processWithOpenAI(text, conversationHistory, detectedLanguage);
        
        if (response) {
          // Update conversation history
          conversationHistory.push(
            { role: "user", content: text },
            { role: "assistant", content: response }
          );

          // Keep only last 10 messages
          if (conversationHistory.length > 10) {
            conversationHistory = conversationHistory.slice(-10);
          }

          // Step 3: Synthesize with Sarvam
          await synthesizeWithSarvam(response, detectedLanguage, ws, sessionId);
        }

        console.log(`‚úÖ [PROCESSING] Completed in ${timer.end()}ms`);
      } catch (error) {
        console.error(`‚ùå [PROCESSING] Error: ${error.message}`);
        timer.end();
      } finally {
        isProcessing = false;
      }
    };

    // Handle Deepgram responses
    const handleDeepgramResponse = async (data) => {
      if (data.type === "Results") {
        const channel = data.channel;
        if (channel && channel.alternatives && channel.alternatives.length > 0) {
          const transcript = channel.alternatives[0].transcript;
          const confidence = channel.alternatives[0].confidence;
          const is_final = data.is_final;
          if (transcript && transcript.trim()) {
            // Log transcript from Deepgram
            console.log(`üìù [DEEPGRAM->TEXT] Transcript: "${transcript}" (final: ${is_final}, confidence: ${confidence})`);
            if (is_final) {
              console.log(`üé§ [TRANSCRIPT] Final: "${transcript}" (confidence: ${confidence})`);
              // Add to buffer and process
              userUtteranceBuffer += (userUtteranceBuffer ? " " : "") + transcript.trim();
              await processUserUtterance(userUtteranceBuffer);
              userUtteranceBuffer = "";
            } else {
              // Send interim results to client
              if (ws.readyState === WebSocket.OPEN) {
                ws.send(JSON.stringify({
                  type: "transcript_interim",
                  data: transcript,
                  confidence: confidence,
                  is_final: false,
                  language: currentLanguage,
                }));
              }
            }
          }
        }
      } else if (data.type === "SpeechStarted") {
        console.log("üéôÔ∏è [DEEPGRAM] Speech started");
      } else if (data.type === "UtteranceEnd") {
        console.log("üîö [DEEPGRAM] Utterance ended");
        if (userUtteranceBuffer.trim()) {
          await processUserUtterance(userUtteranceBuffer);
          userUtteranceBuffer = "";
        }
      }
    };

    // Send audio to Deepgram
    const sendAudioToDeepgram = (audioData) => {
      if (!deepgramWs || deepgramWs.readyState !== WebSocket.OPEN || !deepgramReady) {
        // Buffer audio until Deepgram is ready
        deepgramAudioBufferQueue.push(audioData);
        console.warn("‚ö†Ô∏è [MEDIA] Deepgram not ready, buffering audio chunk (queue length:", deepgramAudioBufferQueue.length, ")");
        return false;
      }
      try {
        const buffer = audioData instanceof Buffer ? audioData : Buffer.from(audioData);
        if (buffer.length >= 320) {
          deepgramWs.send(buffer);
          return true;
        }
        return false;
      } catch (error) {
        console.error("‚ùå [DEEPGRAM] Send error:", error.message);
        return false;
      }
    };

    // WebSocket message handling
    ws.on("message", async (message) => {
      const messageTimer = createTimer("MESSAGE_PROCESSING");

      try {
        let isTextMessage = false;
        let data = null;

        // Check if message is text/JSON
        if (typeof message === "string") {
          isTextMessage = true;
          try {
            data = JSON.parse(message);
          } catch (parseError) {
            console.error("‚ùå Failed to parse JSON:", parseError.message);
            messageTimer.end();
            return;
          }
        } else if (message instanceof Buffer) {
          try {
            const messageStr = message.toString("utf8");
            if (messageStr.trim().startsWith("{") && messageStr.trim().endsWith("}")) {
              data = JSON.parse(messageStr);
              isTextMessage = true;
            } else {
              isTextMessage = false;
            }
          } catch (parseError) {
            isTextMessage = false;
          }
        }

        if (isTextMessage && data) {
          // Handle 'media' event: decode and forward audio to Deepgram
          if (data.event === "media" && data.media && data.media.payload) {
            const audioBuffer = Buffer.from(data.media.payload, "base64");
            console.log(`üéß [MEDIA] Received media event: chunk=${data.media.chunk}, bytes=${audioBuffer.length}`);
            if (audioBuffer.length >= 320) {
              const sent = sendAudioToDeepgram(audioBuffer);
              if (sent) {
                console.log("‚úÖ [MEDIA] Sent audio buffer to Deepgram");
              } else {
                console.warn("‚ö†Ô∏è [MEDIA] Failed to send audio buffer to Deepgram");
              }
            } else {
              console.warn(`‚ö†Ô∏è [MEDIA] Audio buffer too small for Deepgram: ${audioBuffer.length} bytes`);
            }
            // Set outboundStreamSid if not already set
            if (!outboundStreamSid && data.streamSid) {
              outboundStreamSid = data.streamSid;
              outboundSequenceNumber = 1;
              outboundChunkNumber = 1;
              console.log(`üîó [SESSION] Set outboundStreamSid: ${outboundStreamSid}`);
            }
            messageTimer.end();
            return;
          }
          console.log(`üì® [MESSAGE] Received:`, data);

          // Handle different message types
          if (data.event === "start" && data.session_id) {
            sessionId = data.session_id;
            console.log(`üéØ [SESSION] Started: ${sessionId}`);
            // Set outboundStreamSid if present
            if (data.streamSid) {
              outboundStreamSid = data.streamSid;
              outboundSequenceNumber = 1;
              outboundChunkNumber = 1;
              console.log(`üîó [SESSION] Set outboundStreamSid: ${outboundStreamSid}`);
            }
            // Send session confirmation
            if (ws.readyState === WebSocket.OPEN) {
              ws.send(JSON.stringify({
                type: "session_started",
                session_id: sessionId,
                agent_name: DEFAULT_CONFIG.agentName,
                language: currentLanguage,
                timestamp: new Date().toISOString(),
              }));
            }
            // Connect to Deepgram and send greeting
            await connectToDeepgram();
            await sendInitialGreeting();

          } else if (data.type === "synthesize") {
            // Direct synthesis request
            if (data.session_id) {
              sessionId = data.session_id;
            }
            await synthesizeWithSarvam(data.text, data.language || currentLanguage, ws, sessionId);

          } else if (data.data && data.data.hangup === "true") {
            console.log(`üìû [SESSION] Hangup for session ${sessionId}`);
            
            if (deepgramWs && deepgramWs.readyState === WebSocket.OPEN) {
              deepgramWs.close(1000, "Call ended");
            }
            
            ws.close(1000, "Hangup requested");
          }
        } else {
          // This is audio data - send to Deepgram
          sendAudioToDeepgram(message);
        }

        messageTimer.end();
      } catch (error) {
        console.error(`‚ùå [MESSAGE] Processing error: ${error.message}`);
        messageTimer.end();
      }
    });

    // Connection cleanup
    ws.on("close", () => {
      console.log(`üîó [SESSION] Connection closed for session ${sessionId}`);
      
      if (deepgramWs && deepgramWs.readyState === WebSocket.OPEN) {
        deepgramWs.close(1000, "Session ended");
      }

      // Reset all state
      sessionId = null;
      currentLanguage = "hi";
      detectedLanguage = "hi";
      conversationHistory = [];
      isProcessing = false;
      userUtteranceBuffer = "";
      lastProcessedText = "";
      deepgramReady = false;
      outboundStreamSid = null;
      outboundSequenceNumber = 1;
      outboundChunkNumber = 1;
      deepgramAudioBufferQueue = [];
    });

    ws.on("error", (error) => {
      console.error(`‚ùå [SESSION] WebSocket error: ${error.message}`);
    });
  });
};

module.exports = { setupUnifiedVoiceServer  };

// Example usage:
// const WebSocket = require('ws');
// const wss = new WebSocket.Server({ port: 8080 });
// setupUnifiedVoiceServer (wss);
// console.log('üöÄ Server running on ws://localhost:8080');
