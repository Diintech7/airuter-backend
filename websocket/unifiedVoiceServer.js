const WebSocket = require("ws")
const FormData = require("form-data")

// Use native fetch (Node.js 18+) or fallback
const fetch = globalThis.fetch || require("node-fetch")

if (!fetch) {
  console.error("‚ùå Fetch not available. Please use Node.js 18+ or install node-fetch@2")
  process.exit(1)
}

const setupUnifiedVoiceServer = (wss) => {
  console.log("üöÄ Unified Voice WebSocket server initialized")

  wss.on("connection", (ws, req) => {
    console.log("üîó New unified voice connection established")

    // Deepgram client state
    let deepgramWs = null
    let deepgramReady = false
    let audioBuffer = []
    let deepgramConnected = false

    // Rate limiting state
    let audioQueue = []
    let isProcessingQueue = false
    let lastSentTime = 0
    const MIN_SEND_INTERVAL = 250 // Minimum 250ms between sends to Deepgram
    const MAX_QUEUE_SIZE = 50 // Maximum queued audio chunks
    let reconnectAttempts = 0
    const MAX_RECONNECT_ATTEMPTS = 5
    let reconnectDelay = 1000 // Start with 1 second

    // LMNT client state
    const lmntApiKey = process.env.LMNT_API_KEY

    // Session state
    let sessionId = null
    let audioChunkCount = 0
    let connectionGreetingSent = false

    // Extract language from URL parameters
    const url = new URL(req.url, "http://localhost")
    const language = url.searchParams.get("language") || "hi"

    console.log(`üåê Connection established with language: ${language}`)
    console.log(`üîë TTS API Key configured: ${lmntApiKey ? "Yes (" + lmntApiKey.substring(0, 8) + "...)" : "‚ùå NO"}`)

    // Default greeting messages based on language
    const getGreetingMessage = (lang) => {
      const greetings = {
        'hi': '‡§®‡§Æ‡§∏‡•ç‡§§‡•á! ‡§Æ‡•à‡§Ç ‡§Ü‡§™‡§ï‡•Ä ‡§∏‡§π‡§æ‡§Ø‡§§‡§æ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ø‡§π‡§æ‡§Å ‡§π‡•Ç‡§Å‡•§ ‡§Ü‡§™ ‡§Æ‡•Å‡§ù‡§∏‡•á ‡§ï‡•Å‡§õ ‡§≠‡•Ä ‡§™‡•Ç‡§õ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç‡•§',
        'en': 'Hi! Hello, how can I help you today? Feel free to ask me anything.',
        'es': '¬°Hola! ¬øC√≥mo puedo ayudarte hoy?',
        'fr': 'Bonjour! Comment puis-je vous aider aujourd\'hui?',
        'de': 'Hallo! Wie kann ich Ihnen heute helfen?',
        'it': 'Ciao! Come posso aiutarti oggi?',
        'pt': 'Ol√°! Como posso ajud√°-lo hoje?',
        'ja': '„Åì„Çì„Å´„Å°„ÅØÔºÅ‰ªäÊó•„ÅØ„Å©„ÅÆ„Çà„ÅÜ„Å´„ÅäÊâã‰ºù„ÅÑ„Åß„Åç„Åæ„Åô„ÅãÔºü',
        'ko': 'ÏïàÎÖïÌïòÏÑ∏Ïöî! Ïò§Îäò Ïñ¥ÎñªÍ≤å ÎèÑÏôÄÎìúÎ¶¥ÍπåÏöî?',
        'zh': '‰Ω†Â•ΩÔºÅÊàë‰ªäÂ§©ÂèØ‰ª•Â¶Ç‰ΩïÂ∏ÆÂä©ÊÇ®Ôºü',
        'ar': 'ŸÖÿ±ÿ≠ÿ®ÿß! ŸÉŸäŸÅ ŸäŸÖŸÉŸÜŸÜŸä ŸÖÿ≥ÿßÿπÿØÿ™ŸÉ ÿßŸÑŸäŸàŸÖÿü',
        'ru': '–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ —è –º–æ–≥—É –ø–æ–º–æ—á—å –≤–∞–º —Å–µ–≥–æ–¥–Ω—è?'
      }
      return greetings[lang] || greetings['en']
    }

    // Convert buffer to Python-like bytes string representation
    const bufferToPythonBytesString = (buffer) => {
      let result = "b'"
      for (let i = 0; i < buffer.length; i++) {
        const byte = buffer[i]
        if (byte >= 32 && byte <= 126 && byte !== 92 && byte !== 39) {
          // Printable ASCII characters (except backslash and single quote)
          result += String.fromCharCode(byte)
        } else {
          // Non-printable characters as hex escape sequences
          result += '\\x' + byte.toString(16).padStart(2, '0')
        }
      }
      result += "'"
      return result
    }

    // Function to send default greeting
    const sendGreeting = async () => {
      if (connectionGreetingSent || !lmntApiKey) {
        return
      }

      console.log("üëã ==================== SENDING CONNECTION GREETING ====================")
      console.log("üëã Language:", language)
      
      const greetingText = getGreetingMessage(language)
      console.log("üëã Greeting text:", greetingText)

      try {
        // Generate session ID if not exists
        if (!sessionId) {
          sessionId = generateSessionId()
        }

        const synthesisOptions = {
          voice: "lily",
          language: language === 'hi' ? 'hi' : 'en', // LMNT might not support all languages
          speed: 1.0,
        }

        console.log("üëã Synthesizing greeting with options:", synthesisOptions)
        const audioData = await synthesizeWithErrorHandling(greetingText, synthesisOptions)

        if (!audioData || audioData.length === 0) {
          throw new Error("Received empty greeting audio data from TTS")
        }

        console.log("‚úÖ Greeting: Successfully received audio data, size:", audioData.length, "bytes")

        // Convert audio to the required format with raw bytes
        const audioBuffer = Buffer.from(audioData)
        const audioWithHeader = createWAVHeader(audioBuffer, 8000, 1, 16)
        const pythonBytesString = bufferToPythonBytesString(audioWithHeader)

        // Increment chunk count
        audioChunkCount++

        // Send greeting audio in the required format with raw bytes
        const greetingResponse = {
          data: {
            session_id: sessionId,
            count: audioChunkCount,
            audio_bytes_to_play: pythonBytesString,
            sample_rate: 8000,
            channels: 1,
            sample_width: 2,
          },
          type: "greeting"
        }

        console.log("‚úÖ ==================== SENDING GREETING AUDIO ====================")
        console.log("‚úÖ Greeting session_id:", greetingResponse.data.session_id)
        console.log("‚úÖ Greeting count:", greetingResponse.data.count)
        console.log("‚úÖ Greeting audio bytes preview:", pythonBytesString.substring(0, 100) + "...")

        if (ws.readyState === WebSocket.OPEN) {
          ws.send(JSON.stringify(greetingResponse))
          console.log("‚úÖ üëã Connection greeting sent successfully!")
          connectionGreetingSent = true
        } else {
          console.error("‚ùå WebSocket not open, cannot send greeting")
        }

      } catch (error) {
        console.error("‚ùå ==================== GREETING ERROR ====================")
        console.error("‚ùå Failed to send greeting:", error.message)
        console.error("‚ùå Full error:", error)
        
        // Don't send error to client for greeting failure, just log it
        // The connection should still work normally
        connectionGreetingSent = true // Mark as sent to avoid retrying
      }
    }

    // Audio queue processor with rate limiting
    const processAudioQueue = async () => {
      if (isProcessingQueue || audioQueue.length === 0) {
        return
      }

      isProcessingQueue = true

      while (audioQueue.length > 0 && deepgramReady && deepgramWs && deepgramWs.readyState === WebSocket.OPEN) {
        const now = Date.now()
        const timeSinceLastSend = now - lastSentTime

        // Enforce minimum interval between sends
        if (timeSinceLastSend < MIN_SEND_INTERVAL) {
          const waitTime = MIN_SEND_INTERVAL - timeSinceLastSend
          console.log(`‚è±Ô∏è Rate limiting: waiting ${waitTime}ms before next send`)
          await new Promise((resolve) => setTimeout(resolve, waitTime))
        }

        const audioData = audioQueue.shift()
        const success = await sendAudioToDeepgramThrottled(audioData)

        if (!success) {
          // If send failed, put the audio back at the front of the queue
          audioQueue.unshift(audioData)
          break
        }

        lastSentTime = Date.now()

        // Small delay between chunks to prevent overwhelming
        await new Promise((resolve) => setTimeout(resolve, 50))
      }

      isProcessingQueue = false

      // Continue processing if there are more items in queue
      if (audioQueue.length > 0) {
        setTimeout(processAudioQueue, MIN_SEND_INTERVAL)
      }
    }

    // Enhanced audio sending with better error handling
    const sendAudioToDeepgramThrottled = async (audioData) => {
      if (!deepgramWs) {
        console.error("‚ùå STT: Cannot send audio - WebSocket not initialized")
        return false
      }

      if (deepgramWs.readyState !== WebSocket.OPEN) {
        console.error("‚ùå STT: Cannot send audio - WebSocket not open, current state:", deepgramWs.readyState)
        return false
      }

      if (!deepgramReady) {
        console.error("‚ùå STT: Cannot send audio - Client not ready")
        return false
      }

      try {
        const buffer = audioData instanceof Buffer ? audioData : Buffer.from(audioData)
        console.log("üéµ STT: Sending audio data, size:", buffer.length, "bytes")

        deepgramWs.send(buffer)
        return true
      } catch (error) {
        console.error("‚ùå STT: Error sending audio data:", error)

        // If it's a rate limiting error, we should back off
        if (error.message.includes("429") || error.message.includes("rate limit")) {
          console.log("üö´ Rate limit detected, backing off...")
          await new Promise((resolve) => setTimeout(resolve, 2000))
        }

        return false
      }
    }

    // Queue audio data with overflow protection
    const queueAudioData = (audioData) => {
      // Prevent queue overflow
      if (audioQueue.length >= MAX_QUEUE_SIZE) {
        console.warn("‚ö†Ô∏è Audio queue full, dropping oldest chunk")
        audioQueue.shift() // Remove oldest chunk
      }

      audioQueue.push(audioData)
      console.log(`üìä Audio queued. Queue size: ${audioQueue.length}`)

      // Start processing if not already running
      if (!isProcessingQueue) {
        processAudioQueue()
      }
    }

    // Deepgram WebSocket connection function with exponential backoff
    const connectToDeepgram = async (options = {}) => {
      return new Promise((resolve, reject) => {
        try {
          console.log("üéôÔ∏è Connecting to STT with options:", JSON.stringify(options))
          console.log("üéôÔ∏è STT API Key present:", !!process.env.DEEPGRAM_API_KEY)
          console.log(
            "üéôÔ∏è STT API Key preview:",
            process.env.DEEPGRAM_API_KEY ? process.env.DEEPGRAM_API_KEY.substring(0, 12) + "..." : "MISSING",
          )

          if (!process.env.DEEPGRAM_API_KEY) {
            const error = "STT API key not configured"
            console.error("‚ùå STT:", error)
            reject(new Error(error))
            return
          }

          // Build Deepgram WebSocket URL
          const deepgramUrl = new URL("wss://api.deepgram.com/v1/listen")
          deepgramUrl.searchParams.append("sample_rate", "16000")
          deepgramUrl.searchParams.append("channels", "1")
          deepgramUrl.searchParams.append("interim_results", "true")
          deepgramUrl.searchParams.append("language", options.language || "en")
          deepgramUrl.searchParams.append("model", "nova-2")
          deepgramUrl.searchParams.append("smart_format", "true")
          deepgramUrl.searchParams.append("punctuate", "true")
          deepgramUrl.searchParams.append("diarize", "false")

          console.log("üéôÔ∏è STT URL:", deepgramUrl.toString())

          deepgramWs = new WebSocket(deepgramUrl.toString(), ["token", process.env.DEEPGRAM_API_KEY])
          deepgramWs.binaryType = "arraybuffer"

          // Add connection timeout with exponential backoff
          const connectionTimeout = setTimeout(() => {
            console.error("‚ùå STT: Connection timeout after 15 seconds")
            if (deepgramWs) {
              deepgramWs.close()
            }
            reject(new Error("STT connection timeout"))
          }, 15000)

          deepgramWs.onopen = () => {
            clearTimeout(connectionTimeout)
            console.log("‚úÖ STT: WebSocket connection established successfully")
            deepgramReady = true
            deepgramConnected = true
            reconnectAttempts = 0 // Reset reconnect attempts on successful connection
            reconnectDelay = 1000 // Reset delay

            // Process any queued audio
            console.log("üéôÔ∏è STT: Processing", audioQueue.length, "queued audio chunks")
            if (audioQueue.length > 0) {
              processAudioQueue()
            }

            resolve()
          }

          deepgramWs.onmessage = (event) => {
            try {
              const rawData = typeof event.data === "string" ? event.data : Buffer.from(event.data).toString()
              console.log("üéôÔ∏è STT: Raw message received:", rawData.substring(0, 500))

              const data = JSON.parse(rawData)
              console.log("üéôÔ∏è STT: Parsed data:", JSON.stringify(data, null, 2))

              // Check for different types of Deepgram responses
              if (data.type === "Results") {
                console.log("üéôÔ∏è STT: Received Results message")

                if (data.channel?.alternatives?.[0]?.transcript) {
                  const transcript = data.channel.alternatives[0].transcript
                  const confidence = data.channel.alternatives[0].confidence
                  const is_final = data.is_final

                  console.log("üìù STT: Found transcript:", transcript)
                  console.log("üìù STT: Confidence:", confidence)
                  console.log("üìù STT: Is final:", is_final)

                  // Enhanced console logging for STT text
                  if (transcript.trim()) {
                    console.log("üó£Ô∏è ==================== STT TRANSCRIPT ====================")
                    console.log("üó£Ô∏è SPEECH TO TEXT:", transcript)
                    console.log("üó£Ô∏è CONFIDENCE SCORE:", (confidence * 100).toFixed(2) + "%")
                    console.log("üó£Ô∏è STATUS:", is_final ? "FINAL" : "INTERIM")
                    console.log("üó£Ô∏è LANGUAGE:", language)
                    console.log("üó£Ô∏è TIMESTAMP:", new Date().toISOString())
                    console.log("üó£Ô∏è =====================================================")

                    console.log("üì§ STT: Sending transcript to client:", transcript)
                    if (ws.readyState === WebSocket.OPEN) {
                      ws.send(
                        JSON.stringify({
                          type: "transcript",
                          data: transcript,
                          confidence: confidence,
                          is_final: is_final,
                          language: language,
                        }),
                      )
                    }
                  }
                }
              } else if (data.type === "Metadata") {
                console.log("üéôÔ∏è STT: Received Metadata:", JSON.stringify(data, null, 2))
              } else if (data.type === "SpeechStarted") {
                console.log("üéôÔ∏è STT: Speech started detected")
                console.log("üó£Ô∏è ==================== SPEECH DETECTION ====================")
                console.log("üó£Ô∏è USER STARTED SPEAKING")
                console.log("üó£Ô∏è TIMESTAMP:", new Date().toISOString())
                console.log("üó£Ô∏è =====================================================")
              } else if (data.type === "UtteranceEnd") {
                console.log("üéôÔ∏è STT: Utterance end detected")
                console.log("üó£Ô∏è ==================== SPEECH DETECTION ====================")
                console.log("üó£Ô∏è USER STOPPED SPEAKING")
                console.log("üó£Ô∏è TIMESTAMP:", new Date().toISOString())
                console.log("üó£Ô∏è =====================================================")
              } else {
                console.log("üéôÔ∏è STT: Unknown message type:", data.type)
              }
            } catch (parseError) {
              console.error("‚ùå STT: Error parsing message:", parseError)
              console.error("‚ùå STT: Raw data:", event.data)
            }
          }

          deepgramWs.onerror = (error) => {
            clearTimeout(connectionTimeout)
            console.error("‚ùå STT: WebSocket error:", error)
            deepgramReady = false
            deepgramConnected = false

            // Check if it's a rate limiting error
            if (error.message && error.message.includes("429")) {
              console.log("üö´ STT: Rate limit error detected")
              // Send rate limit error to client
              if (ws.readyState === WebSocket.OPEN) {
                ws.send(
                  JSON.stringify({
                    type: "error",
                    error: "Rate limit exceeded. Please slow down audio transmission.",
                  }),
                )
              }
            }

            reject(error)
          }

          deepgramWs.onclose = (event) => {
            clearTimeout(connectionTimeout)
            console.log(`üéôÔ∏è STT: Connection closed with code ${event.code}, reason: ${event.reason}`)
            deepgramReady = false
            deepgramConnected = false

            // Handle rate limiting (429) or other recoverable errors
            if (event.code === 1006 || event.code === 1011 || event.reason.includes("429")) {
              console.log("üîÑ STT: Attempting to reconnect due to recoverable error...")

              if (reconnectAttempts < MAX_RECONNECT_ATTEMPTS) {
                reconnectAttempts++
                const delay = Math.min(reconnectDelay * Math.pow(2, reconnectAttempts - 1), 30000) // Max 30 seconds

                console.log(
                  `üîÑ STT: Reconnect attempt ${reconnectAttempts}/${MAX_RECONNECT_ATTEMPTS} in ${delay}ms`,
                )

                setTimeout(() => {
                  connectToDeepgram(options).catch((err) => {
                    console.error("‚ùå STT: Reconnection failed:", err)
                  })
                }, delay)
              } else {
                console.error("‚ùå STT: Max reconnection attempts reached")
                if (ws.readyState === WebSocket.OPEN) {
                  ws.send(
                    JSON.stringify({
                      type: "error",
                      error: "STT connection failed after multiple attempts. Please refresh and try again.",
                    }),
                  )
                }
              }
            }
          }
        } catch (error) {
          console.error("‚ùå STT: Error during setup:", error)
          reject(error)
        }
      })
    }

    // Close Deepgram connection function
    const closeDeepgram = () => {
      console.log("üéôÔ∏è STT: Closing connection")
      deepgramReady = false
      deepgramConnected = false
      audioQueue = [] // Clear queue
      isProcessingQueue = false

      if (deepgramWs) {
        try {
          deepgramWs.close(1000, "Client closing")
          console.log("‚úÖ STT: WebSocket closed successfully")
        } catch (error) {
          console.error("‚ùå STT: Error closing WebSocket:", error)
        }
      }
    }

    // LMNT synthesis function with comprehensive error handling and multiple API approaches
    const synthesizeWithLMNT = async (text, options = {}) => {
      console.log("üîä ==================== TTS SYNTHESIS START ====================")
      console.log("üîä TTS: Starting synthesis for text:", text.substring(0, 100) + "...")
      console.log("üîä TTS: API Key present:", !!lmntApiKey)
      console.log("üîä TTS: API Key preview:", lmntApiKey ? lmntApiKey.substring(0, 12) + "..." : "MISSING")

      if (!lmntApiKey) {
        const error = "TTS API key not configured in environment variables"
        console.error("‚ùå TTS:", error)
        throw new Error(error)
      }

      const synthesisOptions = {
        voice: options.voice || "lily",
        language: options.language || "en",
        speed: options.speed || 1.0,
        format: "wav",
        sample_rate: 8000,
      }

      console.log("üîä TTS: Final synthesis options:", JSON.stringify(synthesisOptions, null, 2))

      // Try multiple LMNT API approaches
      const apiAttempts = [
        {
          name: "LMNT v1/ai/speech (JSON)",
          url: "https://api.lmnt.com/v1/ai/speech",
          method: "json",
        },
        {
          name: "LMNT v1/ai/speech (FormData)",
          url: "https://api.lmnt.com/v1/ai/speech",
          method: "form",
        },
      ]

      for (const attempt of apiAttempts) {
        try {
          console.log(`üîä TTS: Trying ${attempt.name}...`)

          const requestOptions = {
            method: "POST",
            headers: {
              "X-API-Key": lmntApiKey,
            },
          }

          if (attempt.method === "json") {
            requestOptions.headers["Content-Type"] = "application/json"
            requestOptions.body = JSON.stringify({
              text: text,
              voice: synthesisOptions.voice,
              format: synthesisOptions.format,
              language: synthesisOptions.language,
              sample_rate: synthesisOptions.sample_rate,
              speed: synthesisOptions.speed,
            })
          } else if (attempt.method === "form") {
            const form = new FormData()
            form.append("text", text)
            form.append("voice", synthesisOptions.voice)
            form.append("format", synthesisOptions.format)
            form.append("language", synthesisOptions.language)
            form.append("sample_rate", synthesisOptions.sample_rate.toString())
            form.append("speed", synthesisOptions.speed.toString())

            requestOptions.headers = {
              ...requestOptions.headers,
              ...form.getHeaders(),
            }
            requestOptions.body = form
          }

          console.log(`üîä TTS: Making ${attempt.method.toUpperCase()} request to:`, attempt.url)

          const response = await fetch(attempt.url, requestOptions)

          console.log(`üîä TTS: Response status:`, response.status)

          if (!response.ok) {
            const errorText = await response.text()
            console.error(`‚ùå TTS: ${attempt.name} failed with status ${response.status}:`, errorText)
            continue // Try next approach
          }

          const contentType = response.headers.get("content-type")
          console.log(`üîä TTS: Response content-type:`, contentType)

          if (contentType && contentType.includes("application/json")) {
            // Handle JSON response
            const jsonResponse = await response.json()
            console.log(`üîä TTS: JSON response:`, JSON.stringify(jsonResponse, null, 2))

            if (jsonResponse.audio_url) {
              console.log(`üîä TTS: Fetching audio from URL:`, jsonResponse.audio_url)
              const audioResponse = await fetch(jsonResponse.audio_url)
              if (!audioResponse.ok) {
                throw new Error(`Failed to fetch audio from URL: ${audioResponse.status}`)
              }
              const audioBuffer = await audioResponse.arrayBuffer()
              console.log(`‚úÖ TTS: Downloaded audio buffer, size:`, audioBuffer.byteLength, "bytes")
              return Buffer.from(audioBuffer)
            } else if (jsonResponse.audio) {
              // Direct audio data in JSON
              console.log(`üîä TTS: Found direct audio data in JSON response`)
              const audioBuffer = Buffer.from(jsonResponse.audio, "base64")
              console.log(`‚úÖ TTS: Decoded audio buffer, size:`, audioBuffer.length, "bytes")
              return audioBuffer
            } else {
              throw new Error("Unexpected JSON response format: " + JSON.stringify(jsonResponse))
            }
          } else {
            // Handle binary audio response
            const audioBuffer = await response.arrayBuffer()
            console.log(`‚úÖ TTS: Received binary audio buffer, size:`, audioBuffer.byteLength, "bytes")

            if (audioBuffer.byteLength === 0) {
              throw new Error("TTS returned empty audio buffer")
            }

            console.log(`‚úÖ TTS: Successfully got audio from ${attempt.name}`)
            return Buffer.from(audioBuffer)
          }
        } catch (error) {
          console.error(`‚ùå TTS: ${attempt.name} failed:`, error.message)

          // If this is the last attempt, throw the error
          if (attempt === apiAttempts[apiAttempts.length - 1]) {
            throw error
          }

          continue // Try next approach
        }
      }

      throw new Error("All TTS API attempts failed")
    }

    // Enhanced synthesis wrapper with comprehensive error handling
    const synthesizeWithErrorHandling = async (text, options = {}) => {
      console.log("üîä ==================== SYNTHESIS WRAPPER START ====================")

      try {
        // Send immediate acknowledgment to client
        if (ws.readyState === WebSocket.OPEN) {
          ws.send(
            JSON.stringify({
              type: "synthesis_started",
              text: text.substring(0, 50) + "...",
              status: "processing",
            }),
          )
        }

        const result = await synthesizeWithLMNT(text, options)

        console.log("‚úÖ Synthesis wrapper: Success, audio size:", result.length)
        return result
      } catch (error) {
        console.error("‚ùå ==================== SYNTHESIS WRAPPER ERROR ====================")
        console.error("‚ùå Synthesis wrapper error:", error.message)
        console.error("‚ùå Full error:", error)
        console.error("‚ùå Stack:", error.stack)

        // Send error to client
        if (ws.readyState === WebSocket.OPEN) {
          ws.send(
            JSON.stringify({
              type: "synthesis_error",
              error: error.message,
              details: error.stack,
            }),
          )
        }

        throw error
      }
    }

    // Convert browser audio to PCM format
    const convertToPCM = async (audioBuffer) => {
      try {
        // Browser audio is typically already in the right format
        // Just ensure it's a proper buffer
        return audioBuffer instanceof Buffer ? audioBuffer : Buffer.from(audioBuffer)
      } catch (error) {
        console.error("‚ùå Error converting audio to PCM:", error)
        return audioBuffer
      }
    }

    // Create WAV header for audio data
    const createWAVHeader = (audioBuffer, sampleRate = 8000, channels = 1, bitsPerSample = 16) => {
      const byteRate = (sampleRate * channels * bitsPerSample) / 8
      const blockAlign = (channels * bitsPerSample) / 8
      const dataSize = audioBuffer.length
      const fileSize = 36 + dataSize

      const header = Buffer.alloc(44)
      let offset = 0

      // RIFF header
      header.write("RIFF", offset)
      offset += 4
      header.writeUInt32LE(fileSize, offset)
      offset += 4
      header.write("WAVE", offset)
      offset += 4

      // fmt chunk
      header.write("fmt ", offset)
      offset += 4
      header.writeUInt32LE(16, offset)
      offset += 4
      header.writeUInt16LE(1, offset)
      offset += 2 // audio format (PCM)
      header.writeUInt16LE(channels, offset)
      offset += 2
      header.writeUInt32LE(sampleRate, offset)
      offset += 4
      header.writeUInt32LE(byteRate, offset)
      offset += 4
      header.writeUInt16LE(blockAlign, offset)
      offset += 2
      header.writeUInt16LE(bitsPerSample, offset)
      offset += 2

      // data chunk
      header.write("data", offset)
      offset += 4
      header.writeUInt32LE(dataSize, offset)

      return Buffer.concat([header, audioBuffer])
    }

    // Generate session ID
    const generateSessionId = () => {
      return "session_" + Date.now() + "_" + Math.random().toString(36).substr(2, 9)
    }

    // Handle incoming messages
    ws.on("message", async (message) => {
      try {
        console.log("üì® ==================== MESSAGE RECEIVED ====================")
        console.log("üì® Received message, type:", typeof message, "length:", message.length)

        // Check if message is binary (audio data) or text (JSON commands)
        let isTextMessage = false
        let data = null

        if (typeof message === "string") {
          // Definitely a text message
          isTextMessage = true
          try {
            data = JSON.parse(message)
          } catch (parseError) {
            console.error("‚ùå Failed to parse string message as JSON:", parseError)
            return
          }
        } else if (message instanceof Buffer) {
          // Could be text or binary - try to parse as JSON first
          try {
            const messageStr = message.toString("utf8")

            // Check if it looks like JSON
            if (messageStr.trim().startsWith("{") && messageStr.trim().endsWith("}")) {
              data = JSON.parse(messageStr)
              isTextMessage = true
              console.log("‚úÖ Successfully parsed buffer as JSON text message")
            } else {
              // Doesn't look like JSON, treat as binary audio
              isTextMessage = false
              console.log("üéµ Buffer doesn't look like JSON, treating as binary audio data")
            }
          } catch (parseError) {
            // Failed to parse as JSON, treat as binary audio data
            isTextMessage = false
            console.log("üéµ Failed to parse buffer as JSON, treating as binary audio data")
          }
        }

        if (isTextMessage && data) {
          console.log("üìù Processing text message")
          console.log("üìù Parsed JSON data:", JSON.stringify(data, null, 2))
          if (data.type === "start" && data.uuid) {
            console.log("üöÄ Processing START command")
            // Handle session start
            sessionId = data.uuid
            audioChunkCount = 0
            console.log("‚úÖ Session started with ID:", sessionId)
    
            if (ws.readyState === WebSocket.OPEN) {
              ws.send(
                JSON.stringify({
                  type: "session_started",
                  session_id: sessionId,
                  language: language,
                }),
              )
            }
    
            // Send greeting after session start (with a small delay to ensure client is ready)
            setTimeout(() => {
              sendGreeting()
            }, 1000)
    
          } else if (data.type === "synthesize") {
            console.log("üîä ==================== PROCESSING SYNTHESIZE COMMAND ====================")
            console.log("üîä Synthesis text:", data.text)
    
            try {
              const synthesisOptions = {
                voice: data.voice || "lily",
                language: data.language || language,
                speed: data.speed || 1.0,
              }
    
              const audioData = await synthesizeWithErrorHandling(data.text, synthesisOptions)
    
              if (!audioData || audioData.length === 0) {
                throw new Error("Received empty audio data from TTS")
              }
    
              console.log("‚úÖ TTS: Successfully received audio data, size:", audioData.length, "bytes")
    
              // Convert audio to the required format with raw bytes
              const audioBuffer = Buffer.from(audioData)
              const audioWithHeader = createWAVHeader(audioBuffer, 8000, 1, 16)
              const pythonBytesString = bufferToPythonBytesString(audioWithHeader)
    
              // Increment chunk count
              audioChunkCount++
    
              // Send audio in the required format with raw bytes
              const audioResponse = {
                data: {
                  session_id: sessionId || generateSessionId(),
                  count: audioChunkCount,
                  audio_bytes_to_play: pythonBytesString,
                  sample_rate: 8000,
                  channels: 1,
                  sample_width: 2,
                },
              }
    
              console.log("‚úÖ ==================== SENDING AUDIO RESPONSE ====================")
              console.log("‚úÖ Sending audio response with session_id:", audioResponse.data.session_id)
              console.log("‚úÖ Count:", audioResponse.data.count)
              console.log("‚úÖ Audio bytes preview:", pythonBytesString.substring(0, 100) + "...")
    
              if (ws.readyState === WebSocket.OPEN) {
                ws.send(JSON.stringify(audioResponse))
                console.log("‚úÖ Audio sent in JSON format successfully")
              } else {
                console.error("‚ùå WebSocket not open, cannot send audio response")
              }
            } catch (error) {
              console.error("‚ùå ==================== SYNTHESIS ERROR ====================")
              console.error("‚ùå Synthesis error:", error.message)
    
              if (ws.readyState === WebSocket.OPEN) {
                ws.send(
                  JSON.stringify({
                    type: "error",
                    error: `Speech synthesis failed: ${error.message}`,
                  }),
                )
              }
            }
          }
        } else {
          console.log("üéµ Processing binary message (audio data), size:", message.length)
    
          // This is audio data for transcription
          console.log("üéôÔ∏è Received audio data for transcription, size:", message.length)
    
          // Initialize Deepgram if not already connected
          if (!deepgramConnected) {
            console.log("üéôÔ∏è Initializing STT connection...")
            try {
              await connectToDeepgram({
                language: language,
                model: "nova-2",
                punctuate: true,
                diarize: false,
                tier: "enhanced",
              })
              console.log("‚úÖ STT connection established")
            } catch (error) {
              console.error("‚ùå Failed to connect to STT:", error)
              if (ws.readyState === WebSocket.OPEN) {
                ws.send(
                  JSON.stringify({
                    type: "error",
                    error: "Failed to initialize transcription service: " + error.message,
                  }),
                )
              }
              return
            }
          }
    
          // Convert browser audio to PCM format
          const pcmAudio = await convertToPCM(message)
    
          // Queue the audio data instead of sending immediately
          queueAudioData(pcmAudio)
        }
      } catch (error) {
        console.error("‚ùå ==================== MESSAGE PROCESSING ERROR ====================")
        console.error("‚ùå Error processing message:", error.message)
        console.error("‚ùå Full error:", error)
    
        if (ws.readyState === WebSocket.OPEN) {
          ws.send(
            JSON.stringify({
              type: "error",
              error: error.message,
            }),
          )
        }
      }
    })
    
    // Handle connection close
    ws.on("close", () => {
      console.log("üîó Unified voice connection closed")
    
      // Clean up Deepgram connection
      closeDeepgram()
    
      // Reset state
      sessionId = null
      audioChunkCount = 0
      audioBuffer = []
      audioQueue = []
      deepgramReady = false
      deepgramConnected = false
      isProcessingQueue = false
      connectionGreetingSent = false
    })
    
    // Handle connection errors
    ws.on("error", (error) => {
      console.error("‚ùå WebSocket error:", error)
    })
    
    // Send connection confirmation
    if (ws.readyState === WebSocket.OPEN) {
      ws.send(
        JSON.stringify({
          type: "connected",
          language: language,
          services: ["transcription", "synthesis"],
          lmnt_configured: !!lmntApiKey,
          rate_limiting: {
            min_send_interval: MIN_SEND_INTERVAL,
            max_queue_size: MAX_QUEUE_SIZE,
          },
        }),
      )
    
      // Send greeting after a short delay to ensure client is ready
      setTimeout(() => {
        sendGreeting()
      }, 1500)
    }
  })
}

module.exports = { setupUnifiedVoiceServer }