const WebSocket = require("ws")
const FormData = require("form-data")
const fs = require("fs")
const path = require("path")

// Use native fetch (Node.js 18+) or fallback
const fetch = globalThis.fetch || require("node-fetch")

if (!fetch) {
  console.error("‚ùå Fetch not available. Please use Node.js 18+ or install node-fetch@2")
  process.exit(1)
}

const setupUnifiedVoiceServer = (wss) => {
  console.log("üöÄ Unified Voice WebSocket server initialized")

  wss.on("connection", (ws, req) => {
    console.log("üîó New unified voice connection established")
    console.log("üì° SIP Connection Details:", {
      timestamp: new Date().toISOString(),
      clientIP: req.socket.remoteAddress,
      userAgent: req.headers["user-agent"],
      origin: req.headers.origin,
    })

    // Deepgram client state
    let deepgramWs = null
    let deepgramReady = false
    let audioBuffer = [] // Buffer for audio before Deepgram is connected
    let deepgramConnected = false

    // Audio file management
    let audioFiles = []
    let currentAudioFile = null
    let audioFileIndex = 0
    let emptyAudioFileCount = 0
    const MAX_AUDIO_FILES = 60
    const AUDIO_FILES_DIR = path.join(__dirname, 'audio_chunks')
    
    // Create audio directory if it doesn't exist
    if (!fs.existsSync(AUDIO_FILES_DIR)) {
      fs.mkdirSync(AUDIO_FILES_DIR, { recursive: true })
    }

    // Voice Activity Detection
    let vadState = {
      isSpeaking: false,
      silenceCount: 0,
      speechCount: 0,
      lastAudioLevel: 0,
      SILENCE_THRESHOLD: 5, // Number of consecutive silent chunks
      SPEECH_THRESHOLD: 3,  // Number of consecutive speech chunks
      AUDIO_LEVEL_THRESHOLD: 0.01 // Minimum audio level to consider as speech
    }

    // Rate limiting state for Deepgram
    let audioQueue = []
    let isProcessingQueue = false
    let lastSentTime = 0
    const MIN_SEND_INTERVAL = 250 // Minimum 250ms between sends to Deepgram
    const MAX_QUEUE_SIZE = 50 // Maximum queued audio chunks
    const MAX_BUFFER_SIZE = 100 // Maximum buffered audio chunks before Deepgram connects
    let reconnectAttempts = 0
    const MAX_RECONNECT_ATTEMPTS = 5
    let reconnectDelay = 1000 // Start with 1 second

    // LMNT client state
    const lmntApiKey = process.env.LMNT_API_KEY

    // Session state
    let sessionId = null
    let audioChunkCount = 0
    let connectionGreetingSent = false
    let sipDataReceived = 0

    // Voicebot conversation state
    let currentTranscript = ""
    let emptyAudioCount = 0
    const SILENCE_THRESHOLD = 10 // Number of consecutive empty/non-final Deepgram results to trigger TTS
    let isSpeaking = false // Flag to track if user is actively speaking

    // Extract language from URL parameters
    const url = new URL(req.url, "http://localhost")
    const language = url.searchParams.get("language") || "en"

    console.log(`üåê Connection established with language: ${language}`)
    console.log(`üîë TTS API Key configured: ${lmntApiKey ? "Yes (" + lmntApiKey.substring(0, 8) + "...)" : "‚ùå NO"}`)

    // Voice Activity Detection function
    const detectVoiceActivity = (audioBuffer) => {
      if (!audioBuffer || audioBuffer.length === 0) {
        return false
      }

      // Convert buffer to 16-bit PCM samples
      const samples = []
      for (let i = 0; i < audioBuffer.length; i += 2) {
        if (i + 1 < audioBuffer.length) {
          const sample = audioBuffer.readInt16LE(i)
          samples.push(sample)
        }
      }

      if (samples.length === 0) {
        return false
      }

      // Calculate RMS (Root Mean Square) for audio level
      let sum = 0
      for (const sample of samples) {
        sum += sample * sample
      }
      const rms = Math.sqrt(sum / samples.length)
      const audioLevel = rms / 32768.0 // Normalize to 0-1 range

      vadState.lastAudioLevel = audioLevel
      
      // Determine if this chunk contains speech
      const hasSpeech = audioLevel > vadState.AUDIO_LEVEL_THRESHOLD

      if (hasSpeech) {
        vadState.speechCount++
        vadState.silenceCount = 0
        
        if (vadState.speechCount >= vadState.SPEECH_THRESHOLD && !vadState.isSpeaking) {
          vadState.isSpeaking = true
          console.log(`üé§ Speech detected! Audio level: ${audioLevel.toFixed(4)}`)
        }
      } else {
        vadState.silenceCount++
        vadState.speechCount = 0
        
        if (vadState.silenceCount >= vadState.SILENCE_THRESHOLD && vadState.isSpeaking) {
          vadState.isSpeaking = false
          console.log(`üîá Silence detected! Audio level: ${audioLevel.toFixed(4)}`)
        }
      }

      return hasSpeech
    }

    // Save audio chunk to file
    const saveAudioChunk = (audioData, hasVoice = false) => {
      try {
        const filename = `audio_chunk_${audioFileIndex.toString().padStart(3, '0')}.wav`
        const filepath = path.join(AUDIO_FILES_DIR, filename)
        
        // Create WAV file with proper header
        const audioBuffer = Buffer.from(audioData)
        const audioWithHeader = createWAVHeader(audioBuffer, 8000, 1, 16)
        
        fs.writeFileSync(filepath, audioWithHeader)
        
        const fileInfo = {
          index: audioFileIndex,
          filename: filename,
          filepath: filepath,
          hasVoice: hasVoice,
          timestamp: new Date().toISOString(),
          size: audioWithHeader.length
        }
        
        audioFiles.push(fileInfo)
        audioFileIndex++
        
        if (hasVoice) {
          console.log(`üíæ Audio saved: ${filename} (${audioWithHeader.length} bytes) - CONTAINS SPEECH`)
        } else {
          emptyAudioFileCount++
          console.log(`üíæ Audio saved: ${filename} (${audioWithHeader.length} bytes) - SILENT`)
        }
        
        // Clean up old files if we exceed the limit
        if (audioFiles.length > MAX_AUDIO_FILES) {
          const oldFile = audioFiles.shift()
          try {
            fs.unlinkSync(oldFile.filepath)
            console.log(`üóëÔ∏è Deleted old audio file: ${oldFile.filename}`)
          } catch (error) {
            console.log(`‚ö†Ô∏è Failed to delete old audio file: ${error.message}`)
          }
        }
        
        return fileInfo
      } catch (error) {
        console.log(`‚ùå Failed to save audio chunk: ${error.message}`)
        return null
      }
    }

    // Remove empty audio files
    const removeEmptyAudioFiles = () => {
      const removedFiles = []
      audioFiles = audioFiles.filter(fileInfo => {
        if (!fileInfo.hasVoice) {
          try {
            fs.unlinkSync(fileInfo.filepath)
            removedFiles.push(fileInfo.filename)
            console.log(`üóëÔ∏è Removed empty audio file: ${fileInfo.filename}`)
            return false
          } catch (error) {
            console.log(`‚ö†Ô∏è Failed to remove empty audio file: ${error.message}`)
            return true
          }
        }
        return true
      })
      
      if (removedFiles.length > 0) {
        console.log(`üßπ Removed ${removedFiles.length} empty audio files`)
      }
    }

    // Concatenate audio files into one
    const concatenateAudioFiles = () => {
      return new Promise((resolve, reject) => {
        try {
          if (audioFiles.length === 0) {
            console.log("‚ö†Ô∏è No audio files to concatenate")
            resolve(null)
            return
          }

          // Filter only files with voice
          const voiceFiles = audioFiles.filter(file => file.hasVoice)
          
          if (voiceFiles.length === 0) {
            console.log("‚ö†Ô∏è No voice files to concatenate")
            resolve(null)
            return
          }

          console.log(`üîó Concatenating ${voiceFiles.length} voice files...`)
          
          // Read all voice files and concatenate their audio data (without headers)
          const audioChunks = []
          let totalSize = 0
          
          for (const fileInfo of voiceFiles) {
            const fileBuffer = fs.readFileSync(fileInfo.filepath)
            // Skip WAV header (44 bytes) and get only audio data
            const audioData = fileBuffer.slice(44)
            audioChunks.push(audioData)
            totalSize += audioData.length
          }
          
          // Concatenate all audio data
          const combinedAudio = Buffer.concat(audioChunks)
          
          // Create final WAV file with proper header
          const finalAudioWithHeader = createWAVHeader(combinedAudio, 8000, 1, 16)
          
          const finalFilename = `concatenated_audio_${Date.now()}.wav`
          const finalFilepath = path.join(AUDIO_FILES_DIR, finalFilename)
          
          fs.writeFileSync(finalFilepath, finalAudioWithHeader)
          
          console.log(`‚úÖ Audio concatenated: ${finalFilename} (${finalAudioWithHeader.length} bytes)`)
          
          resolve({
            filepath: finalFilepath,
            filename: finalFilename,
            audioData: combinedAudio,
            totalFiles: voiceFiles.length,
            totalSize: finalAudioWithHeader.length
          })
          
        } catch (error) {
          console.log(`‚ùå Failed to concatenate audio files: ${error.message}`)
          reject(error)
        }
      })
    }

    // Process accumulated audio and send to Deepgram
    const processAccumulatedAudio = async () => {
      try {
        // Remove empty audio files first
        removeEmptyAudioFiles()
        
        // Concatenate remaining voice files
        const concatenatedAudio = await concatenateAudioFiles()
        
        if (!concatenatedAudio) {
          console.log("‚ö†Ô∏è No audio to process")
          return
        }
        
        console.log(`üì§ Sending concatenated audio to Deepgram: ${concatenatedAudio.totalSize} bytes`)
        
        // Send concatenated audio to Deepgram
        if (deepgramWs && deepgramWs.readyState === WebSocket.OPEN && deepgramReady) {
          deepgramWs.send(concatenatedAudio.audioData)
          console.log("‚úÖ Concatenated audio sent to Deepgram")
        } else {
          console.log("‚ùå Cannot send to Deepgram - connection not ready")
        }
        
        // Clean up processed files
        audioFiles = []
        audioFileIndex = 0
        emptyAudioFileCount = 0
        
      } catch (error) {
        console.log(`‚ùå Failed to process accumulated audio: ${error.message}`)
      }
    }

    // Enhanced SIP data logging function
    const logSipData = (data, type = "UNKNOWN") => {
      const timestamp = new Date().toISOString()
      sipDataReceived++

      console.log("=".repeat(80))
      console.log(`üìû SIP TEAM DATA RECEIVED [${sipDataReceived}] - ${timestamp}`)
      console.log("=".repeat(80))
      console.log(`üîç Data Type: ${type}`)
      console.log(`üìä Data Size: ${typeof data === "string" ? data.length : data.byteLength || "unknown"} bytes`)

      if (typeof data === "string") {
        console.log(`üìù Text Content: ${data.substring(0, 200)}${data.length > 200 ? "..." : ""}`)
      } else if (data instanceof Buffer) {
        console.log(`üéµ Audio Buffer: ${data.length} bytes`)
        console.log(`üéµ Audio Preview: ${data.toString("hex").substring(0, 40)}...`)
      }

      console.log(`üîó Session ID: ${sessionId || "Not set"}`)
      console.log(`üåç Language: ${language}`)
      console.log("=".repeat(80))

      // Send real-time notification to client about SIP data
      if (ws.readyState === WebSocket.OPEN) {
        ws.send(
          JSON.stringify({
            type: "sip_data_received",
            timestamp: timestamp,
            dataType: type,
            dataSize: typeof data === "string" ? data.length : data.byteLength || 0,
            sessionId: sessionId,
            count: sipDataReceived,
          }),
        )
      }
    }

    // Default greeting messages based on language
    const getGreetingMessage = (lang) => {
      const greetings = {
        hi: "‡§®‡§Æ‡§∏‡•ç‡§§‡•á! ‡§π‡•à‡§≤‡•ã, Aitota ‡§∏‡•á ‡§∏‡§Ç‡§™‡§∞‡•ç‡§ï ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶‡•§",
        en: "Hi! Hello, thank you for contacting Aitota.",
        es: "¬°Hola! Gracias por contactar con Aitota.",
        fr: "Bonjour ! Merci de contacter Aitota.",
        de: "Hallo! Danke, dass Sie Aitota kontaktiert haben.",
        it: "Ciao! Grazie per aver contattato Aitota.",
        pt: "Ol√°! Obrigado por entrar em contato com a Aitota.",
        ja: "„Åì„Çì„Å´„Å°„ÅØÔºÅAitota „Å´„ÅîÈÄ£Áµ°„ÅÑ„Åü„Å†„Åç„ÅÇ„Çä„Åå„Å®„ÅÜ„Åî„Åñ„ÅÑ„Åæ„Åô„ÄÇ",
        ko: "ÏïàÎÖïÌïòÏÑ∏Ïöî! AitotaÏóê Ïó∞ÎùΩÌï¥ Ï£ºÏÖîÏÑú Í∞êÏÇ¨Ìï©ÎãàÎã§„ÄÇ",
        zh: "‰Ω†Â•ΩÔºÅÊÑüË∞¢ÊÇ®ËÅîÁ≥ª Aitota„ÄÇ",
        ar: "ŸÖÿ±ÿ≠ÿ®Ÿãÿß! ÿ¥ŸÉÿ±Ÿãÿß ŸÑÿ™ŸàÿßÿµŸÑŸÉ ŸÖÿπ Aitota.",
        ru: "–ü—Ä–∏–≤–µ—Ç! –°–ø–∞—Å–∏–±–æ, —á—Ç–æ –æ–±—Ä–∞—Ç–∏–ª–∏—Å—å –≤ Aitota.",
      }

      return greetings[lang] || greetings["en"]
    }

    // Convert buffer to Python-like bytes string representation
    const bufferToPythonBytesString = (buffer) => {
      let result = "b'"
      for (let i = 0; i < buffer.length; i++) {
        const byte = buffer[i]
        if (byte >= 32 && byte <= 126 && byte !== 92 && byte !== 39) {
          // Printable ASCII characters (except backslash and single quote)
          result += String.fromCharCode(byte)
        } else {
          // Non-printable characters as hex escape sequences
          result += "\\x" + byte.toString(16).padStart(2, "0")
        }
      }
      result += "'"
      return result
    }

    // Function to send default greeting
    const sendGreeting = async () => {
      if (connectionGreetingSent || !lmntApiKey || !sessionId) {
        console.log("‚ö†Ô∏è Skipping greeting: already sent, no API key, or no session ID.")
        return
      }

      const greetingText = getGreetingMessage(language)
      console.log("üëã Greeting text:", greetingText)

      try {
        const synthesisOptions = {
          voice: "lily",
          language: language === "en" ? "en" : "hi", // LMNT might not support all languages
          speed: 1.0,
        }

        const audioData = await synthesizeWithErrorHandling(greetingText, synthesisOptions)

        if (!audioData || audioData.length === 0) {
          throw new Error("Received empty greeting audio data from TTS")
        }

        console.log("‚úÖ Greeting: Successfully received audio data, size:", audioData.length, "bytes")

        // Convert audio to the required format with raw bytes
        const audioBuffer = Buffer.from(audioData)
        const audioWithHeader = createWAVHeader(audioBuffer, 8000, 1, 16)
        const pythonBytesString = bufferToPythonBytesString(audioWithHeader)

        // Increment chunk count
        audioChunkCount++

        // Send greeting audio in the required format with raw bytes
        const greetingResponse = {
          data: {
            session_id: sessionId,
            count: audioChunkCount,
            audio_bytes_to_play: pythonBytesString,
            sample_rate: 8000,
            channels: 1,
            sample_width: 2,
          },
          type: "greeting",
        }

        console.log("‚úÖ ==================== SENDING GREETING AUDIO ====================")
        console.log("‚úÖ Greeting session_id:", greetingResponse.data.session_id)
        console.log("‚úÖ Greeting count:", greetingResponse.data.count)
        console.log("‚úÖ Greeting audio bytes preview:", pythonBytesString.substring(0, 100) + "...")

        if (ws.readyState === WebSocket.OPEN) {
          ws.send(JSON.stringify(greetingResponse))
          console.log("‚úÖ üëã Connection greeting sent successfully!")
          connectionGreetingSent = true
        } else {
          console.log("‚ùå WebSocket not open, cannot send greeting")
        }
      } catch (error) {
        console.log("‚ùå Failed to send greeting:", error.message)
        connectionGreetingSent = true // Mark as sent to avoid retrying
        // Do NOT send error to SIP client
      }
    }

    // Audio queue processor with rate limiting
    const processAudioQueue = async () => {
      if (isProcessingQueue || audioQueue.length === 0) {
        console.log("‚è© Skipping audio queue processing: already running or queue is empty.")
        return
      }

      isProcessingQueue = true
      console.log(`üéµ Starting audio queue processing. Queue size: ${audioQueue.length}`)

      while (audioQueue.length > 0 && deepgramReady && deepgramWs && deepgramWs.readyState === WebSocket.OPEN) {
        const now = Date.now()
        const timeSinceLastSend = now - lastSentTime

        // Enforce minimum interval between sends
        if (timeSinceLastSend < MIN_SEND_INTERVAL) {
          const waitTime = MIN_SEND_INTERVAL - timeSinceLastSend
          console.log(`‚è≥ Waiting ${waitTime}ms before sending next audio chunk to Deepgram.`)
          await new Promise((resolve) => setTimeout(resolve, waitTime))
        }

        const audioData = audioQueue.shift()
        const success = await sendAudioToDeepgramThrottled(audioData)

        if (!success) {
          // If send failed, put the audio back at the front of the queue
          audioQueue.unshift(audioData)
          console.log("‚ö†Ô∏è Failed to send audio to Deepgram, re-queueing and pausing processing.")
          break
        }

        lastSentTime = Date.now()

        // Small delay between chunks to prevent overwhelming
        await new Promise((resolve) => setTimeout(resolve, 50))
      }

      isProcessingQueue = false
      console.log("üéµ Audio queue processing finished.")

      // Continue processing if there are more items in queue
      if (audioQueue.length > 0) {
        console.log("üéµ More audio in queue, scheduling next processing cycle.")
        setTimeout(processAudioQueue, MIN_SEND_INTERVAL)
      }
    }

    // Enhanced audio sending with better error handling
    const sendAudioToDeepgramThrottled = async (audioData) => {
      if (!deepgramWs) {
        console.log("‚ö†Ô∏è Deepgram WebSocket not available for sending.")
        return false
      }

      if (deepgramWs.readyState !== WebSocket.OPEN) {
        console.log("‚ö†Ô∏è Deepgram WebSocket not open for sending, state:", deepgramWs.readyState)
        return false
      }

      if (!deepgramReady) {
        console.log("‚ö†Ô∏è Deepgram not ready for sending.")
        return false
      }

      try {
        const buffer = audioData instanceof Buffer ? audioData : Buffer.from(audioData)
        console.log(`üéµ Sending ${buffer.length} bytes to Deepgram for transcription.`)
        deepgramWs.send(buffer)
        return true
      } catch (error) {
        console.log("‚ùå Error sending audio to Deepgram:", error.message)

        // If it's a rate limiting error, we should back off
        if (error.message.includes("429") || error.message.includes("rate limit")) {
          console.log("‚è≥ Rate limit detected, backing off...")
          await new Promise((resolve) => setTimeout(resolve, 2000))
        }
        // Do NOT send error to SIP client
        return false
      }
    }

    // Queue audio data with overflow protection
    const queueAudioData = (audioData) => {
      // Prevent queue overflow
      if (audioQueue.length >= MAX_QUEUE_SIZE) {
        const removed = audioQueue.shift() // Remove oldest chunk
        console.log(
          `‚ö†Ô∏è Audio queue overflow, removed oldest chunk (${removed.length} bytes). Current queue size: ${audioQueue.length}`,
        )
      }

      audioQueue.push(audioData)
      console.log(`üéµ Audio queued: ${audioData.length} bytes, current queue size: ${audioQueue.length}`)

      // Start processing if not already running
      if (!isProcessingQueue) {
        processAudioQueue()
      }
    }

    // Deepgram WebSocket connection function with exponential backoff
    const connectToDeepgram = async (options = {}) => {
      return new Promise((resolve, reject) => {
        try {
          console.log("üéôÔ∏è Connecting to Deepgram STT service...")

          if (!process.env.DEEPGRAM_API_KEY) {
            const error = "STT API key not configured"
            console.log("‚ùå", error)
            reject(new Error(error))
            return
          }

          // Build Deepgram WebSocket URL
          const deepgramUrl = new URL("wss://api.deepgram.com/v1/listen")
          deepgramUrl.searchParams.append("sample_rate", "8000") // Explicitly set to 8kHz
          deepgramUrl.searchParams.append("channels", "1")
          deepgramUrl.searchParams.append("interim_results", "true")
          deepgramUrl.searchParams.append("language", options.language || "en")
          deepgramUrl.searchParams.append("model", "nova-2")
          deepgramUrl.searchParams.append("smart_format", "true")
          deepgramUrl.searchParams.append("punctuate", "true")
          deepgramUrl.searchParams.append("diarize", "false")
          deepgramUrl.searchParams.append("encoding", "linear16") // Explicitly set encoding
          deepgramUrl.searchParams.append("endpointing", "300") // Add endpointing for better VAD

          console.log(`üéôÔ∏è Deepgram URL: ${deepgramUrl.toString()}`)

          deepgramWs = new WebSocket(deepgramUrl.toString(), ["token", process.env.DEEPGRAM_API_KEY])

          deepgramWs.binaryType = "arraybuffer"

          // Add connection timeout with exponential backoff
          const connectionTimeout = setTimeout(() => {
            if (deepgramWs) {
              deepgramWs.close()
            }
            reject(new Error("STT connection timeout"))
          }, 15000)

          deepgramWs.onopen = () => {
            clearTimeout(connectionTimeout)
            deepgramReady = true
            deepgramConnected = true
            reconnectAttempts = 0 // Reset reconnect attempts on successful connection
            reconnectDelay = 1000 // Reset delay
            console.log("‚úÖ Deepgram STT connection established")

            // Process any queued audio
            if (audioQueue.length > 0) {
              console.log(`üéµ Processing ${audioQueue.length} queued audio chunks`)
              processAudioQueue()
            }

            resolve()
          }

          deepgramWs.onmessage = async (event) => {
            try {
              const rawData = typeof event.data === "string" ? event.data : Buffer.from(event.data).toString()

              const data = JSON.parse(rawData)
              console.log("üéôÔ∏è Deepgram Raw Message:", JSON.stringify(data, null, 2)) // Detailed log for all Deepgram messages

              // Check for different types of Deepgram responses
              if (data.type === "Results") {
                console.log("üéôÔ∏è STT Result received:", data.type)

                if (data.channel?.alternatives?.[0]?.transcript) {
                  const transcript = data.channel.alternatives[0].transcript
                  const confidence = data.channel.alternatives[0].confidence
                  const is_final = data.is_final

                  if (transcript.trim()) {
                    // Speech detected
                    currentTranscript += (currentTranscript ? " " : "") + transcript.trim()
                    emptyAudioCount = 0 // Reset empty count on valid speech
                    isSpeaking = true
                    console.log("üìù Accumulated Transcript:", currentTranscript) // Log accumulated transcript

                    if (ws.readyState === WebSocket.OPEN) {
                      ws.send(
                        JSON.stringify({
                          type: "transcript",
                          data: transcript,
                          confidence: confidence,
                          is_final: is_final,
                          language: language,
                          accumulated: currentTranscript, // Send accumulated for debugging/display
                        }),
                      )
                    }
                  } else if (is_final) {
                    // Final result with empty transcript means silence or non-speech
                    emptyAudioCount++
                    console.log(
                      `üîá Empty final transcript. Empty audio count: ${emptyAudioCount}. Not adding to accumulated transcript.`,
                    )
                    // If silence threshold reached and we were speaking, trigger TTS
                    if (isSpeaking && emptyAudioCount >= SILENCE_THRESHOLD) {
                      console.log(`üîá Silence detected (${SILENCE_THRESHOLD} empty chunks). Triggering TTS.`)
                      isSpeaking = false // Mark as not speaking to prevent re-trigger
                      await processUserUtterance()
                    }
                  }
                }
              } else if (data.type === "Metadata") {
                console.log("üéôÔ∏è STT Metadata:", data)
              } else if (data.type === "SpeechStarted") {
                console.log("üéôÔ∏è STT: Speech started detected")
                isSpeaking = true
                emptyAudioCount = 0 // Reset on new speech
              } else if (data.type === "UtteranceEnd") {
                console.log("üéôÔ∏è STT: Utterance end detected. Triggering TTS.")
                if (isSpeaking) {
                  // Only trigger if we were actively speaking
                  isSpeaking = false
                  await processUserUtterance()
                } else {
                  console.log("üéôÔ∏è STT: Utterance end detected but not actively speaking, ignoring.")
                }
              } else {
                console.log("üéôÔ∏è STT: Unknown message type:", data.type)
              }
            } catch (parseError) {
              console.log("‚ùå Error parsing STT response:", parseError.message)
              // Do NOT send error to SIP client
            }
          }

          deepgramWs.onerror = (error) => {
            clearTimeout(connectionTimeout)
            deepgramReady = false
            deepgramConnected = false
            console.log("‚ùå Deepgram STT error:", error.message)

            // Check if it's a rate limiting error
            if (error.message && error.message.includes("429")) {
              console.log("‚ö†Ô∏è Rate limit exceeded for STT service")
            }
            // Do NOT send error to SIP client
            reject(error)
          }

          deepgramWs.onclose = (event) => {
            clearTimeout(connectionTimeout)
            deepgramReady = false
            deepgramConnected = false
            console.log(`üéôÔ∏è STT connection closed: ${event.code} - ${event.reason}`)

            // Handle rate limiting (429) or other recoverable errors
            if (event.code === 1006 || event.code === 1011 || event.reason.includes("429")) {
              console.log("üîÑ Attempting to reconnect to STT service...")

              if (reconnectAttempts < MAX_RECONNECT_ATTEMPTS) {
                reconnectAttempts++
                const delay = Math.min(reconnectDelay * Math.pow(2, reconnectAttempts - 1), 30000) // Max 30 seconds
                console.log(`‚è≥ Reconnecting in ${delay}ms (attempt ${reconnectAttempts}/${MAX_RECONNECT_ATTEMPTS})`)

                setTimeout(() => {
                  connectToDeepgram(options).catch((err) => {
                    console.log("‚ùå STT reconnection failed:", err.message)
                    // Do NOT send error to SIP client
                  })
                }, delay)
              } else {
                console.log("‚ùå Max STT reconnection attempts reached")
                // Do NOT send error to SIP client
              }
            }
          }
        } catch (error) {
          console.log("‚ùå Error creating Deepgram connection:", error.message)
          // Do NOT send error to SIP client
          reject(error)
        }
      })
    }

            // Close Deepgram connection function
            const closeDeepgram = () => {
              console.log("üéôÔ∏è STT: Closing connection")
              deepgramReady = false
              deepgramConnected = false
              audioQueue = [] // Clear queue
              isProcessingQueue = false
    
              if (deepgramWs) {
                try {
                  deepgramWs.close(1000, "Client closing")
                  console.log("‚úÖ STT: WebSocket closed successfully")
                } catch (error) {
                  console.log("‚ö†Ô∏è Error closing STT WebSocket:", error.message)
                  // Do NOT send error to SIP client
                }
              }
            }
    
            // LMNT synthesis function with comprehensive error handling and multiple API approaches
            const synthesizeWithLMNT = async (text, options = {}) => {
              console.log("üîä TTS: Starting synthesis for text:", text.substring(0, 100) + "...")
    
              if (!lmntApiKey) {
                const error = "TTS API key not configured in environment variables"
                console.log("‚ùå", error)
                throw new Error(error)
              }
    
              const synthesisOptions = {
                voice: options.voice || "lily",
                language: options.language || "en",
                speed: options.speed || 1.0,
                format: "wav",
                sample_rate: 8000,
              }
    
              console.log("üîä TTS synthesis options:", synthesisOptions)
    
              // Try multiple LMNT API approaches
              const apiAttempts = [
                {
                  name: "LMNT v1/ai/speech (JSON)",
                  url: "https://api.lmnt.com/v1/ai/speech",
                  method: "json",
                },
                {
                  name: "LMNT v1/ai/speech (FormData)",
                  url: "https://api.lmnt.com/v1/ai/speech",
                  method: "form",
                },
              ]
    
              for (const attempt of apiAttempts) {
                try {
                  console.log(`üîä TTS: Trying ${attempt.name}`)
    
                  const requestOptions = {
                    method: "POST",
                    headers: {
                      "X-API-Key": lmntApiKey,
                    },
                  }
    
                  if (attempt.method === "json") {
                    requestOptions.headers["Content-Type"] = "application/json"
                    requestOptions.body = JSON.stringify({
                      text: text,
                      voice: synthesisOptions.voice,
                      format: synthesisOptions.format,
                      language: synthesisOptions.language,
                      sample_rate: synthesisOptions.sample_rate,
                      speed: synthesisOptions.speed,
                    })
                  } else if (attempt.method === "form") {
                    const form = new FormData()
                    form.append("text", text)
                    form.append("voice", synthesisOptions.voice)
                    form.append("format", synthesisOptions.format)
                    form.append("language", synthesisOptions.language)
                    form.append("sample_rate", synthesisOptions.sample_rate.toString())
                    form.append("speed", synthesisOptions.speed.toString())
    
                    requestOptions.headers = {
                      ...requestOptions.headers,
                      ...form.getHeaders(),
                    }
                    requestOptions.body = form
                  }
    
                  console.log(`üîä TTS: Making request to ${attempt.url}`)
    
                  const response = await fetch(attempt.url, requestOptions)
    
                  console.log(`üîä TTS: Response status: ${response.status}`)
    
                  if (!response.ok) {
                    const errorText = await response.text()
                    console.log(`‚ùå TTS: ${attempt.name} failed:`, errorText)
                    continue // Try next approach
                  }
    
                  const contentType = response.headers.get("content-type")
                  console.log(`üîä TTS: Response content-type: ${contentType}`)
    
                  if (contentType && contentType.includes("application/json")) {
                    // Handle JSON response
                    const jsonResponse = await response.json()
                    console.log("üîä TTS: Received JSON response")
    
                    if (jsonResponse.audio_url) {
                      console.log("üîä TTS: Fetching audio from URL:", jsonResponse.audio_url)
                      const audioResponse = await fetch(jsonResponse.audio_url)
                      if (!audioResponse.ok) {
                        throw new Error(`Failed to fetch audio from URL: ${audioResponse.status}`)
                      }
                      const audioBuffer = await audioResponse.arrayBuffer()
                      console.log(`‚úÖ TTS: Audio fetched from URL, size: ${audioBuffer.byteLength} bytes`)
                      return Buffer.from(audioBuffer)
                    } else if (jsonResponse.audio) {
                      // Direct audio data in JSON
                      const audioBuffer = Buffer.from(jsonResponse.audio, "base64")
                      console.log(`‚úÖ TTS: Direct audio from JSON, size: ${audioBuffer.length} bytes`)
                      return audioBuffer
                    } else {
                      throw new Error("Unexpected JSON response format: " + JSON.stringify(jsonResponse))
                    }
                  } else {
                    // Handle binary audio response
                    const audioBuffer = await response.arrayBuffer()
    
                    if (audioBuffer.byteLength === 0) {
                      throw new Error("TTS returned empty audio buffer")
                    }
    
                    console.log(`‚úÖ TTS: Successfully got audio from ${attempt.name}, size: ${audioBuffer.byteLength} bytes`)
                    return Buffer.from(audioBuffer)
                  }
                } catch (error) {
                  console.log(`‚ùå TTS: ${attempt.name} failed:`, error.message)
    
                  // If this is the last attempt, throw the error
                  if (attempt === apiAttempts[apiAttempts.length - 1]) {
                    throw error
                  }
    
                  continue // Try next approach
                }
              }
    
              throw new Error("All TTS API attempts failed")
            }
    
            // Enhanced synthesis wrapper with comprehensive error handling
            const synthesizeWithErrorHandling = async (text, options = {}) => {
              console.log("üîä Starting TTS synthesis process...")
    
              try {
                // Send immediate acknowledgment to client
                if (ws.readyState === WebSocket.OPEN) {
                  console.log("üì§ Sending TTS processing notification to client")
                }
    
                const result = await synthesizeWithLMNT(text, options)
    
                console.log("‚úÖ Synthesis wrapper: Success, audio size:", result.length)
                return result
              } catch (error) {
                console.log("‚ùå Synthesis wrapper failed:", error.message)
    
                // Do NOT send error to client
                throw error
              }
            }
    
            // Convert browser audio to PCM format (placeholder, as SIP audio should already be PCM)
            const convertToPCM = async (audioBuffer) => {
              try {
                // Assuming incoming SIP audio is already in a suitable PCM format (e.g., 16-bit, 8kHz/16kHz mono)
                const result = audioBuffer instanceof Buffer ? audioBuffer : Buffer.from(audioBuffer)
                console.log(`üéµ Audio converted to PCM: ${result.length} bytes`)
                return result
              } catch (error) {
                console.log("‚ö†Ô∏è PCM conversion warning:", error.message)
                return audioBuffer
              }
            }
    
            // Create WAV header for audio data
            const createWAVHeader = (audioBuffer, sampleRate = 8000, channels = 1, bitsPerSample = 16) => {
              const byteRate = (sampleRate * channels * bitsPerSample) / 8
              const blockAlign = (channels * bitsPerSample) / 8
              const dataSize = audioBuffer.length
              const fileSize = 36 + dataSize
    
              const header = Buffer.alloc(44)
              let offset = 0
    
              // RIFF header
              header.write("RIFF", offset)
              offset += 4
              header.writeUInt32LE(fileSize, offset)
              offset += 4
              header.write("WAVE", offset)
              offset += 4
    
              // fmt chunk
              header.write("fmt ", offset)
              offset += 4
              header.writeUInt32LE(16, offset)
              offset += 4
              header.writeUInt16LE(1, offset)
              offset += 2 // audio format (PCM)
              header.writeUInt16LE(channels, offset)
              offset += 2
              header.writeUInt32LE(sampleRate, offset)
              offset += 4
              header.writeUInt32LE(byteRate, offset)
              offset += 4
              header.writeUInt16LE(blockAlign, offset)
              offset += 2
              header.writeUInt16LE(bitsPerSample, offset)
              offset += 2
    
              // data chunk
              header.write("data", offset)
              offset += 4
              header.writeUInt32LE(dataSize, offset)
    
              console.log(`üéµ WAV header created: ${sampleRate}Hz, ${channels}ch, ${bitsPerSample}bit, ${dataSize} bytes`)
              return Buffer.concat([header, audioBuffer])
            }
    
            // Generate session ID (fallback if SIP doesn't provide one, but SIP provides it here)
            const generateSessionId = () => {
              const id = "session_" + Date.now() + "_" + Math.random().toString(36).substr(2, 9)
              console.log("üÜî Generated session ID:", id)
              return id
            }
    
            // Function to process the accumulated user utterance and send a TTS response
            const processUserUtterance = async () => {
              if (currentTranscript.trim()) {
                console.log("üß† Processing user utterance:", currentTranscript)
                const responseText = `You said: "${currentTranscript}". How can I help you further?` // Example echo response
                try {
                  const synthesisOptions = {
                    voice: "lily", // Or a dynamic voice based on language
                    language: language === "en" ? "en" : "hi",
                    speed: 1.0,
                  }
                  const audioData = await synthesizeWithErrorHandling(responseText, synthesisOptions)
    
                  if (!audioData || audioData.length === 0) {
                    throw new Error("Received empty audio data from TTS for response")
                  }
    
                  const audioBuffer = Buffer.from(audioData)
                  const audioWithHeader = createWAVHeader(audioBuffer, 8000, 1, 16)
                  const pythonBytesString = bufferToPythonBytesString(audioWithHeader)
    
                  audioChunkCount++
                  const audioResponse = {
                    data: {
                      session_id: sessionId,
                      count: audioChunkCount,
                      audio_bytes_to_play: pythonBytesString,
                      sample_rate: 8000,
                      channels: 1,
                      sample_width: 2,
                    },
                  }
    
                  console.log("üì§ Sending synthesized response audio back to SIP client.")
                  if (ws.readyState === WebSocket.OPEN) {
                    ws.send(JSON.stringify(audioResponse))
                    console.log("‚úÖ Synthesized response audio sent!")
                  }
                } catch (error) {
                  console.log("‚ùå Failed to synthesize and send response audio:", error.message)
                  // Do NOT send error to SIP client
                }
              } else {
                console.log("ü§∑ No transcript to process for TTS response.")
              }
              // Reset for next utterance
              currentTranscript = ""
              emptyAudioCount = 0
              isSpeaking = false
              console.log("üìù Transcript and state reset for next utterance.") // Log transcript reset
            }
    
            // Handle incoming messages
            ws.on("message", async (message) => {
              try {
                console.log("üì® Message received from SIP team")
    
                // Check if message is binary (audio data) or text (JSON commands)
                let isTextMessage = false
                let data = null
    
                if (typeof message === "string") {
                  // Definitely a text message
                  isTextMessage = true
                  logSipData(message, "TEXT_MESSAGE")
                  try {
                    data = JSON.parse(message)
                    console.log("üìã Parsed JSON data:", JSON.stringify(data, null, 2))
                  } catch (parseError) {
                    console.log("‚ùå Failed to parse JSON:", parseError.message)
                    // Do NOT send error to SIP client
                    return
                  }
                } else if (message instanceof Buffer) {
                  // Could be text or binary - try to parse as JSON first
                  try {
                    const messageStr = message.toString("utf8")
    
                    // Check if it looks like JSON
                    if (messageStr.trim().startsWith("{") && messageStr.trim().endsWith("}")) {
                      data = JSON.parse(messageStr)
                      isTextMessage = true
                      logSipData(messageStr, "JSON_BUFFER")
                      console.log("üìã Parsed JSON from buffer:", JSON.stringify(data, null, 2))
                    } else {
                      // Doesn't look like JSON, treat as binary audio
                      isTextMessage = false
                      logSipData(message, "BINARY_AUDIO")
                    }
                  } catch (parseError) {
                    // Failed to parse as JSON, treat as binary audio data
                    isTextMessage = false
                    logSipData(message, "BINARY_AUDIO")
                  }
                }
    
                if (isTextMessage && data) {
                  console.log("üîÑ Processing text/JSON message...")
    
                  if (data.event === "start" && data.session_id) {
                    // Handle session start - use SIP-provided session ID
                    sessionId = data.session_id
                    audioChunkCount = 0
                    currentTranscript = "" // Reset transcript for new session
                    emptyAudioCount = 0
                    isSpeaking = false
                    console.log("‚úÖ SIP Call Started with UUID:", sessionId)
                    console.log("Source:", data.Source, "Destination:", data.Destination) // Log Source and Destination
    
                    if (ws.readyState === WebSocket.OPEN) {
                      ws.send(
                        JSON.stringify({
                          type: "session_started",
                          session_id: sessionId, // Echo back the SIP session ID
                          language: language,
                          message: "SIP call started, connecting to STT and sending greeting.",
                        }),
                      )
                      console.log("üì§ Session started confirmation sent with SIP session ID")
                    }
    
                    if (!deepgramConnected) {
                      console.log("üéôÔ∏è Connecting to Deepgram for STT after SIP start...")
                      try {
                        await connectToDeepgram({
                          language: language,
                          model: "nova-2",
                          punctuate: true,
                          diarize: false,
                          tier: "enhanced",
                        })
                        console.log("‚úÖ Deepgram connection established for STT after SIP start.")
    
                        // Process any buffered audio data that arrived before Deepgram connected
                        if (audioBuffer.length > 0) {
                          console.log(`üéµ Processing ${audioBuffer.length} buffered audio chunks`)
                          for (const audioData of audioBuffer) {
                            const pcmAudio = await convertToPCM(audioData)
                            queueAudioData(pcmAudio)
                          }
                          audioBuffer = [] // Clear buffer after processing
                        }
                      } catch (error) {
                        console.log("‚ùå Failed to initialize Deepgram after SIP start:", error.message)
                        // Do NOT send error to SIP client
                      }
                    } else {
                      console.log("‚úÖ Deepgram already connected for STT.")
                    }
    
                    // Send greeting after a short delay to ensure client is ready and STT is connecting
                    setTimeout(() => {
                      console.log("üëã Sending initial greeting after SIP start...")
                      sendGreeting()
                    }, 500) // Reduced delay
                  } else if (data.type === "synthesize") {
                    console.log("üîä TTS synthesis request received")
                    console.log("üìù Text to synthesize:", data.text)
    
                    // Use SIP-provided session ID if available
                    if (data.session_id) {
                      sessionId = data.session_id
                      console.log("üÜî Using SIP-provided session ID for TTS:", sessionId)
                    }
    
                    try {
                      const synthesisOptions = {
                        voice: data.voice || "lily",
                        language: data.language || language,
                        speed: data.speed || 1.0,
                      }
    
                      console.log("üîä TTS options:", synthesisOptions)
    
                      const audioData = await synthesizeWithErrorHandling(data.text, synthesisOptions)
    
                      if (!audioData || audioData.length === 0) {
                        throw new Error("Received empty audio data from TTS")
                      }
    
                      console.log("‚úÖ TTS: Successfully received audio data, size:", audioData.length, "bytes")
    
                      // Convert audio to the required format with raw bytes
                      const audioBuffer = Buffer.from(audioData)
                      const audioWithHeader = createWAVHeader(audioBuffer, 8000, 1, 16)
                      const pythonBytesString = bufferToPythonBytesString(audioWithHeader)
    
                      // Increment chunk count
                      audioChunkCount++
                      console.log("üìä Audio chunk count incremented to:", audioChunkCount)
    
                      // Send audio with the exact session ID from SIP team
                      const audioResponse = {
                        data: {
                          session_id: sessionId, // Use the SIP-provided session ID
                          count: audioChunkCount,
                          audio_bytes_to_play: pythonBytesString,
                          sample_rate: 8000,
                          channels: 1,
                          sample_width: 2,
                        },
                      }
    
                      console.log("üì§ Sending synthesized audio response:")
                      console.log("   Session ID (from SIP):", audioResponse.data.session_id)
                      console.log("   Count:", audioResponse.data.count)
                      console.log("   Audio bytes preview:", pythonBytesString.substring(0, 50) + "...")
    
                      if (ws.readyState === WebSocket.OPEN) {
                        ws.send(JSON.stringify(audioResponse))
                        console.log("‚úÖ Synthesized audio sent with SIP session ID!")
                      } else {
                        console.log("‚ùå WebSocket not open, cannot send audio")
                      }
                    } catch (error) {
                      console.log("‚ùå TTS synthesis failed:", error.message)
                      // Do NOT send error to SIP client
                    }
                  } else if (data.type === "start_stt") {
                    console.log("üéôÔ∏è STT service start requested (explicitly)")
    
                    // Use the session ID provided by SIP team
                    if (data.session_id) {
                      sessionId = data.session_id
                      console.log("üÜî Using SIP-provided session ID:", sessionId)
                    }
    
                    // Initialize Deepgram only when STT is requested
                    if (!deepgramConnected) {
                      console.log("üéôÔ∏è Connecting to Deepgram for STT...")
                      try {
                        await connectToDeepgram({
                          language: data.language || language,
                          model: "nova-2",
                          punctuate: true,
                          diarize: false,
                          tier: "enhanced",
                        })
                        console.log("‚úÖ Deepgram connection established for STT")
    
                        // Process any buffered audio data
                        if (audioBuffer.length > 0) {
                          console.log(`üéµ Processing ${audioBuffer.length} buffered audio chunks`)
                          for (const audioData of audioBuffer) {
                            const pcmAudio = await convertToPCM(audioData)
                            queueAudioData(pcmAudio)
                          }
                          audioBuffer = [] // Clear buffer after processing
                        }
    
                        // Send confirmation
                        if (ws.readyState === WebSocket.OPEN) {
                          ws.send(
                            JSON.stringify({
                              type: "stt_started",
                              session_id: sessionId,
                              message: "Speech-to-text service activated",
                            }),
                          )
                        }
                      } catch (error) {
                        console.log("‚ùå Failed to initialize Deepgram:", error.message)
                        // Do NOT send error to SIP client
                      }
                    } else {
                      console.log("‚úÖ Deepgram already connected")
                      if (ws.readyState === WebSocket.OPEN) {
                        ws.send(
                          JSON.stringify({
                            type: "stt_ready",
                            session_id: sessionId,
                            message: "Speech-to-text service already active",
                          }),
                        )
                      }
                    }
                  } else if (data.type === "stop_stt") {
                    console.log("üéôÔ∏è STT service stop requested")
                    closeDeepgram()
    
                    if (ws.readyState === WebSocket.OPEN) {
                      ws.send(
                        JSON.stringify({
                          type: "stt_stopped",
                          session_id: sessionId,
                          message: "Speech-to-text service deactivated",
                        }),
                      )
                    }
                  } else if (data.session_id && data.played === "true") {
                    console.log(`‚ñ∂Ô∏è Play Event received for session_id: ${data.session_id}`)
                    // Handle play event if needed, e.g., log or trigger next action
                  } else if (data.data && data.data.hangup === "true") {
                    console.log(`üìû Hangup request received for session_id: ${data.data.session_id}`)
                    ws.close(1000, "Hangup requested by Voicebot")
                  } else if (data.data && data.data.stream_stop === "true") {
                    console.log(`üõë Stream stop request received for session_id: ${data.data.session_id}`)
                    closeDeepgram() // Stop STT streaming
                    // Optionally, close the main WebSocket if stream_stop implies end of call
                    // ws.close(1000, "Stream stop requested by Voicebot");
                  } else {
                    console.log("‚ùì Unknown message type or missing required fields:", data.type || data.event, data)
                  }
                } else {
                  console.log("üéµ Audio data received.")
                  logSipData(message, "BINARY_AUDIO") // Changed type to BINARY_AUDIO for clarity
    
                  // Convert and process the audio data
                  const pcmAudio = await convertToPCM(message)
                  
                  // Detect voice activity
                  const hasVoice = detectVoiceActivity(pcmAudio)
                  
                  // Save audio chunk with voice activity information
                  saveAudioChunk(pcmAudio, hasVoice)
                  
                  // If we've reached the maximum number of audio files, process them
                  if (audioFiles.length >= MAX_AUDIO_FILES) {
                    await processAccumulatedAudio()
                  }
    
                  // If Deepgram is ready, queue audio for transcription
                  if (hasVoice) {
                    if (deepgramConnected && deepgramReady) {
                      queueAudioData(pcmAudio)
                    } else {
                      audioBuffer.push(pcmAudio)
                  
                      if (audioBuffer.length > MAX_BUFFER_SIZE) {
                        audioBuffer.shift()
                        console.log(
                          `‚ö†Ô∏è Audio buffer overflow, removed oldest chunk (before STT connected). Current size: ${audioBuffer.length}`
                        )
                      }
                  
                      console.log(`üéµ Audio buffered: ${audioBuffer.length} chunks stored (waiting for STT)`)
                    }
                  } else {
                    console.log("üîá Skipping silent audio chunk for Deepgram.")
                  }
                  
                }
              } catch (error) {
                console.log("‚ùå Error processing message:", error.message)
                console.log("‚ùå Error stack:", error.stack)
    
                // Do NOT send error to SIP client
              }
            })
    
            // Handle connection close
            ws.on("close", () => {
              console.log("üîó Unified voice connection closed")
              console.log("üìä Session statistics:")
              console.log(`   SIP data received: ${sipDataReceived} messages`)
              console.log(`   Audio chunks processed: ${audioChunkCount}`)
              console.log(`   Session ID: ${sessionId || "Not set"}`)
    
              // Clean up audio files
              try {
                const files = fs.readdirSync(AUDIO_FILES_DIR)
                for (const file of files) {
                  fs.unlinkSync(path.join(AUDIO_FILES_DIR, file))
                }
                console.log(`üßπ Cleaned up ${files.length} audio files`)
              } catch (error) {
                console.log("‚ö†Ô∏è Failed to clean up audio files:", error.message)
              }
    
              // Clean up Deepgram connection
              closeDeepgram()
    
              // Reset state
              sessionId = null
              audioChunkCount = 0
              audioBuffer = []
              audioQueue = []
              deepgramReady = false
              deepgramConnected = false
              isProcessingQueue = false
              connectionGreetingSent = false
              sipDataReceived = 0
              currentTranscript = ""
              emptyAudioCount = 0
              isSpeaking = false
              audioFiles = []
              audioFileIndex = 0
              emptyAudioFileCount = 0
            })
    
            // Handle connection errors
            ws.on("error", (error) => {
              console.log("‚ùå WebSocket connection error:", error.message)
              // Do NOT send error to SIP client
            })
    
            // No initial greeting on connection, it will be sent after SIP 'start' event
            console.log("‚úÖ WebSocket connection confirmed, waiting for SIP 'start' event.")
          })
        }
    
        module.exports = { setupUnifiedVoiceServer }